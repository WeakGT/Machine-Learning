{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWd7ExpN6nN7"
      },
      "source": [
        "# 1. Introduction\n",
        "Welcome to your third lab. In this lab, you will learn how to implement linear classifiers with some numerical data (Age, BMI, and Glucose) for predicting Diabetes_mellitus, which means whether the patient has diabetes(1) or not(0).\n",
        "\n",
        "The dataset contains 25000 records for training set and 5000 for testing set.\n",
        "Each instance has 3 features. The features contain Age, BMI, and Glucose.\n",
        "\n",
        "There are three parts in this lab, including\n",
        "\n",
        "  >Part 1: Implement the Perceptron\n",
        "  >\n",
        "  >Part 2: Implement Linear Discriminant Analysis (LDA)\n",
        "  >\n",
        "  >Part 3: Implement Linear Discriminant Analysis (LDA) classifier **using** Gaussian distributions and MAP estimation\n",
        "\n",
        "Please think about the difference between the three classification methods in this lab. Write down your observations in the report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaWGgSrg7D6D"
      },
      "source": [
        "# 2. Packages\n",
        "All the packages that you need to finish this assignment are listed below.\n",
        "*   numpy : the fundamental package for scientific computing with Python.\n",
        "*   csv: a built-in Python module to handle CSV files for reading and writing tabular data.\n",
        "*   pandas: a powerful data manipulation and analysis library for structured data, offering DataFrame objects for efficient handling of datasets\n",
        "*   sklearn.metrics.f1_score: calculate the f1_score of the prediction\n",
        "\n",
        "⚠️ **WARNING** ⚠️:\n",
        "*   Please do not import any other packages in this lab.\n",
        "*   np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n",
        "\n",
        "❗ **Important** ❗: Please do not change the code outside this code bracket.\n",
        "```\n",
        "### START CODE HERE ###\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73G0TlnM8XXe"
      },
      "source": [
        "## Import packages\n",
        "> Note: You **cannot** import any other package in this lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LrwdtF6S8a91"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Ww4EQOSuP-O"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ###\n",
        "training_dataroot = './lab3_training.csv'\n",
        "testing_dataroot = './lab3_testing.csv'\n",
        "\n",
        "output_path_part1 = './output/lab3_part1.csv'\n",
        "output_path_part2 = './output/lab3_part2.csv'\n",
        "output_path_part3 = './output/lab3_part3.csv'\n",
        "### END CODE HERE ###\n",
        "\n",
        "# The example data that are used to test the calculation functions\n",
        "X_exp = np.array([[-0.6415175074,\t-0.2499581931,\t-0.8246180885],\n",
        "    [-1.00480699,\t-0.4248545394,\t0.1841706489],\n",
        "    [-0.157131531,\t-0.01062601622,\t0.6666348277],\n",
        "    [-1.125903484,\t2.901126986,\t0.3705772634],\n",
        "    [1.174929904,\t-0.5494975593,\t0.2389961238],\n",
        "    [-0.6415175074,\t-0.1696408072,\t0.6447046377],\n",
        "    [-1.852482448,\t-0.2256164258,\t-0.6820718539],\n",
        "    [-0.5809692604,\t0.5687763118,\t0.4363678332],\n",
        "    [0.5088991865,\t0.866833251,\t-1.021989798],\n",
        "    [1.05383341,\t0.4590381523,\t0.5679489729],\n",
        "    [0.5694474336,\t0.9588184702,\t0.6885650176],\n",
        "    [1.174929904,\t0.09218273166,\t-0.7588275187],\n",
        "    [-0.2176797781,\t-0.7169736941,\t-0.6711067589],\n",
        "    [0.9327369159,\t-1.19479095,\t-0.8026878985],\n",
        "    [-1.368096472,\t0.1146424265,\t-0.4518048595],\n",
        "    [-0.5809692604,\t2.12232714,\t0.3815423584],\n",
        "    [-1.428644719,\t2.01637356,\t-0.627246379],\n",
        "    [0.6905439277,\t-0.5538426799,\t-0.7917228036],\n",
        "    [-1.125903484,\t-1.205814174,\t-0.616281284],\n",
        "    [-0.2782280251,\t-0.6836720793,\t-0.8575133734]])\n",
        "y_exp = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QAiMStrnU5A"
      },
      "source": [
        "## Split and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "TzDmrkBRYeg8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(20000, 3) (5000, 3) (5000, 3)\n",
            "(20000,) (5000,)\n"
          ]
        }
      ],
      "source": [
        "# Read input csv to datalist\n",
        "train = pd.read_csv(training_dataroot)\n",
        "test = pd.read_csv(testing_dataroot)\n",
        "\n",
        "# split data\n",
        "def SplitData(data):\n",
        "\n",
        "  X_train = data.iloc[:20000, :3].values  # Get training features\n",
        "  X_val = data.iloc[20000:, :3].values    # Get validation features\n",
        "  y_train = data.iloc[:20000, 3].values   # Get training labels\n",
        "  y_val = data.iloc[20000:, 3].values      # Get validation labels\n",
        "\n",
        "  return X_train, X_val, y_train, y_val\n",
        "X_train, X_val, y_train, y_val = SplitData(train)\n",
        "X_test = test.iloc[:, :3].values\n",
        "\n",
        "def StandardizeData(X_train, X_val, X_test):\n",
        "  ### START CODE HERE ###\n",
        "  # Calculate mean and standard deviation of the training set\n",
        "  mean_train = X_train.mean(axis=0)\n",
        "  std_train = X_train.std(axis=0)\n",
        "\n",
        "  # Standardize the training set\n",
        "  X_train_standardized = (X_train - mean_train) / std_train\n",
        "\n",
        "  # Standardize validation set\n",
        "  X_val_standardized = (X_val - mean_train) / std_train\n",
        "\n",
        "  # Standardize test set\n",
        "  X_test_standardized = (X_test - mean_train) / std_train\n",
        "\n",
        "  return X_train_standardized, X_val_standardized, X_test_standardized\n",
        "  ### END CODE HERE ###\n",
        "X_train, X_val, X_test = StandardizeData(X_train, X_val, X_test)\n",
        "\n",
        "print(type(X_train), type(X_val), type(X_test)) # <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
        "print(X_train.shape, X_val.shape, X_test.shape) # (20000, 3) (5000, 3) (5000, 3)\n",
        "print(y_train.shape, y_val.shape) # (20000,) (5000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omdcu9eUSR6l"
      },
      "source": [
        "# Part 1 - Perceptron\n",
        "In this part, you'll be implementing key components of a Perceptron. Your task is to complete the linear_combination and predict methods within the Perceptron class.\n",
        "\n",
        "Here's what you need to focus on:\n",
        "\n",
        "  >**The linear_combination function:**\n",
        "  >\n",
        "  >* This function calculates the weighted sum of input features.\n",
        "  >* You'll need to use numpy's dot product function (np.dot) to multiply the input features with their corresponding weights.\n",
        "  >* Remember to add the bias term (w_[0]) to your calculation.\n",
        "  >\n",
        "  >\n",
        "  >**The predict function:**\n",
        "  >\n",
        "  >* This function determines the class prediction by calling linear_combination function.\n",
        "  >* Use numpy's where function to implement the step function: return 1 if the weighted sum is greater than or equal to 0, and 0 otherwise.\n",
        "  >\n",
        "  >\n",
        "  >**The fit function:**\n",
        "  >\n",
        "  >* Iterate through the training data.\n",
        "  >* For each sample, use the predict method (which you've already implemented) to calculate ŷ (y_pred), the predicted value.\n",
        "  >* Updating the weights by the weight update formula (Please note that our labels are 0 and 1, so the formula may be a little bit different with the one in the slides):\n",
        "  >>$W_{i} = W_{i} + Δ(W_{i})$, where $Δ(W_{i}) = (y - \\hat y) \\times X_{i} $\n",
        "  >* Remember to update both the feature weights (w_[1:]) and the bias term (w_[0]).\n",
        "  >* Count the number of misclassifications in each iteration.\n",
        "\n",
        "Hints:\n",
        "\n",
        "* The weights (w_) are stored as a numpy array, with w_[0] as the bias term and w_[1:] as the weights for each feature.\n",
        "* The input X is a 2D numpy array where each row represents a sample and each column a feature.\n",
        "\n",
        "Reference: slides L5 p.8-16\n",
        "\n",
        "**Please save the prediction result in a csv file lab3_part1.csv and upload to Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "n7FOtfmHhjjh"
      },
      "outputs": [],
      "source": [
        "class Perceptron(object):\n",
        "\n",
        "  def __init__(self, X, n_iter=1):\n",
        "    # initializing the weights to 0\n",
        "    self.w_ = np.zeros(1 + X.shape[1])\n",
        "    self.errors_ = []\n",
        "    self.n_iter = n_iter\n",
        "\n",
        "  def linear_combination(self, X):\n",
        "    # calculate the sum of the weighted values\n",
        "    ### START CODE HERE ###\n",
        "    return np.dot(X, self.w_[1:].T) + self.w_[0]\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "  def predict(self, X):\n",
        "    # return the predicted value (ŷ) of X\n",
        "    ### START CODE HERE ###\n",
        "    return np.where(self.linear_combination(X) >= 0.0, 1, 0)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    print(\"Weights:\", self.w_)\n",
        "\n",
        "    # training the model n_iter times\n",
        "    for _ in range(self.n_iter):\n",
        "      error = 0\n",
        "\n",
        "      # loop through each input\n",
        "      for xi, yi in zip(X, y):\n",
        "        ### START CODE HERE ###\n",
        "        # calculate ŷ (the predicted value)\n",
        "        y_pred = self.predict(xi)\n",
        "        eta = 1 # learning rate = 0.00001 (best)\n",
        "        if yi != y_pred:\n",
        "          update = (yi - y_pred)\n",
        "          self.w_[1:] += eta * update * xi\n",
        "          self.w_[0] += eta * update\n",
        "\n",
        "        # ŷ != y means mismatches\n",
        "        error += int(yi != y_pred)\n",
        "        ### END CODE HERE ###\n",
        "      print(f\"Errors in epoch {_}:\", error)\n",
        "      print(\"Updated Weights:\", self.w_)\n",
        "\n",
        "      self.errors_.append(error)\n",
        "\n",
        "    return self\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0aarnSyZ1rY"
      },
      "source": [
        "## Use the example data to test the weight caculation\n",
        "\n",
        "Expected output:\n",
        "> Weights: [0. 0. 0. 0.]\n",
        ">\n",
        "> Errors in epoch 0: 5\n",
        ">\n",
        "> Updated Weights: [-1.         -2.02260536  2.0821638  -0.75435559]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "lWY1A2Z3a-fM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights: [0. 0. 0. 0.]\n",
            "Errors in epoch 0: 5\n",
            "Updated Weights: [-1.         -2.02260536  2.0821638  -0.75435559]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.Perceptron at 0x168138580>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# X_exp has been standardized\n",
        "perceptron = Perceptron(X_exp, n_iter=1)\n",
        "perceptron.fit(X_exp, y_exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVTsGFC4-j5i"
      },
      "source": [
        "## Train and validate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ShjMSQal2mS4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights: [0. 0. 0. 0.]\n",
            "Errors in epoch 0: 8009\n",
            "Updated Weights: [-1.          0.73444263 -0.04382962  2.26842216]\n",
            "Errors in epoch 1: 8015\n",
            "Updated Weights: [-2.          1.08567614  1.02196519  2.3730904 ]\n",
            "Errors in epoch 2: 8005\n",
            "Updated Weights: [-1.          0.79831081 -0.42299194  1.8048286 ]\n",
            "Errors in epoch 3: 7999\n",
            "Updated Weights: [-2.          0.25538972  1.03835514  1.71244578]\n",
            "Errors in epoch 4: 8046\n",
            "Updated Weights: [0.         0.12773637 0.11695114 1.0385638 ]\n",
            "Errors in epoch 5: 8055\n",
            "Updated Weights: [-1.          0.73444263  0.06070078  1.54970935]\n",
            "Errors in epoch 6: 8056\n",
            "Updated Weights: [-1.          0.67057444 -0.06813246  2.08089048]\n",
            "Errors in epoch 7: 8082\n",
            "Updated Weights: [-1.        -0.5429211  0.0185557  4.2065669]\n",
            "Errors in epoch 8: 8006\n",
            "Updated Weights: [-1.          0.4151017  -0.589156    2.24843147]\n",
            "Errors in epoch 9: 8043\n",
            "Updated Weights: [-2.          0.38312609  0.85241037  1.51920247]\n",
            "Errors in epoch 10: 7940\n",
            "Updated Weights: [ 0.         -0.57481368  0.08770949  2.81107142]\n",
            "Errors in epoch 11: 8103\n",
            "Updated Weights: [-1.          0.73444263 -0.64181663  2.58636928]\n",
            "Errors in epoch 12: 8029\n",
            "Updated Weights: [-2.          0.70246702  0.80533411  2.57394921]\n",
            "Errors in epoch 13: 8005\n",
            "Updated Weights: [-1.          0.73444263  0.14240043  2.15133385]\n",
            "Errors in epoch 14: 8044\n",
            "Updated Weights: [-1.         -0.86226203  0.33953697  1.38978386]\n",
            "Errors in epoch 15: 8032\n",
            "Updated Weights: [-1.          0.79831081 -0.20156606  1.50401635]\n",
            "Errors in epoch 16: 7976\n",
            "Updated Weights: [-1.          0.92604719 -0.43385012  2.28555704]\n",
            "Errors in epoch 17: 8134\n",
            "Updated Weights: [-1.          0.73444263 -0.28646789  0.70343692]\n",
            "Errors in epoch 18: 8092\n",
            "Updated Weights: [-1.          0.862179   -0.16682697  2.40074147]\n",
            "Errors in epoch 19: 8054\n",
            "Updated Weights: [-1.          0.35123351  1.05687258  1.56398842]\n",
            "Errors in epoch 20: 8050\n",
            "Updated Weights: [-1.          0.73444263 -0.01566309  2.0942176 ]\n",
            "Errors in epoch 21: 8058\n",
            "Updated Weights: [-1.          0.54283807 -0.41291546  1.80673248]\n",
            "Errors in epoch 22: 8087\n",
            "Updated Weights: [-2.         -0.44716033  0.25845728  1.76289847]\n",
            "Errors in epoch 23: 8059\n",
            "Updated Weights: [-1.         -0.41518472  1.13313332  2.38075079]\n",
            "Errors in epoch 24: 8027\n",
            "Updated Weights: [ 0.         -0.38320912 -0.22315153  2.03048267]\n",
            "Errors in epoch 25: 8077\n",
            "Updated Weights: [-1.          0.60670625 -0.06459904  2.09707341]\n",
            "Errors in epoch 26: 8035\n",
            "Updated Weights: [-2.          0.19152153  1.03923048  1.63819466]\n",
            "Errors in epoch 27: 8034\n",
            "Updated Weights: [ 0.         -0.12773637  0.36322414  1.37650161]\n",
            "Errors in epoch 28: 8023\n",
            "Updated Weights: [-1.          0.98991537  0.61571892  2.57589797]\n",
            "Errors in epoch 29: 8084\n",
            "Updated Weights: [-1.         -0.09584379 -0.69095555  1.9799851 ]\n",
            "Errors in epoch 30: 8059\n",
            "Updated Weights: [-2.         -0.06395121 -0.03522928  1.57631872]\n",
            "Errors in epoch 31: 8025\n",
            "Updated Weights: [-1.         -0.15971198  1.05855529  2.14371835]\n",
            "Errors in epoch 32: 8080\n",
            "Updated Weights: [-1.          0.92604719 -0.32898893  1.47926598]\n",
            "Errors in epoch 33: 8097\n",
            "Updated Weights: [-2.         -0.19168759  0.87497404  2.17318353]\n",
            "Errors in epoch 34: 8054\n",
            "Updated Weights: [-2.         -0.38329215  0.80754715  1.82382247]\n",
            "Errors in epoch 35: 8091\n",
            "Updated Weights: [-1.          0.54283807 -0.3135176   2.28555704]\n",
            "Errors in epoch 36: 8043\n",
            "Updated Weights: [-2.          0.57473065  0.6191509   1.60392491]\n",
            "Errors in epoch 37: 8158\n",
            "Updated Weights: [-2.          0.83020339 -0.29246291  2.43020665]\n",
            "Errors in epoch 38: 7991\n",
            "Updated Weights: [-1.          0.67057444 -0.32455441  0.71866792]\n",
            "Errors in epoch 39: 8036\n",
            "Updated Weights: [-1.          0.60670625 -0.66033597  1.56684423]\n",
            "Errors in epoch 40: 8085\n",
            "Updated Weights: [-2.          0.57473065 -0.88564693  2.55205465]\n",
            "Errors in epoch 41: 8033\n",
            "Updated Weights: [-1.00000000e+00  7.34442627e-01 -3.12173393e-04  2.26175860e+00]\n",
            "Errors in epoch 42: 8040\n",
            "Updated Weights: [-1.         -0.15971198 -0.48109537  2.0713711 ]\n",
            "Errors in epoch 43: 8055\n",
            "Updated Weights: [ 0.         -0.57481368  0.05073098  2.75585904]\n",
            "Errors in epoch 44: 8004\n",
            "Updated Weights: [-2.          0.25538972  0.61464532  1.66008922]\n",
            "Errors in epoch 45: 8060\n",
            "Updated Weights: [-2.          0.76633521 -0.21030123  2.53491978]\n",
            "Errors in epoch 46: 8033\n",
            "Updated Weights: [-1.          0.4151017  -0.63495572  1.1318088 ]\n",
            "Errors in epoch 47: 8054\n",
            "Updated Weights: [-1.          0.862179   -0.57629002  1.09658711]\n",
            "Errors in epoch 48: 8070\n",
            "Updated Weights: [-1.          0.60670625  0.21506923  2.05042848]\n",
            "Errors in epoch 49: 8097\n",
            "Updated Weights: [-2.          0.57473065 -0.83487392  0.99563685]\n",
            "Errors in epoch 50: 8038\n",
            "Updated Weights: [-2.          0.44699428 -0.47972604  1.48969241]\n",
            "Errors in epoch 51: 8039\n",
            "Updated Weights: [-1.         -0.60678928  0.45073554  2.14086254]\n",
            "Errors in epoch 52: 8035\n",
            "Updated Weights: [-2.         -0.25555577  0.70645044  2.1598564 ]\n",
            "Errors in epoch 53: 8078\n",
            "Updated Weights: [ 0.         -0.57481368  0.01891636  2.74538773]\n",
            "Errors in epoch 54: 8025\n",
            "Updated Weights: [-1.          0.79831081  0.19069209  2.31411516]\n",
            "Errors in epoch 55: 8101\n",
            "Updated Weights: [0.00000000e+00 1.51434421e-13 2.04175560e-01 1.97717417e+00]\n",
            "Errors in epoch 56: 8016\n",
            "Updated Weights: [-2.          0.38312609  0.08531093  1.71530159]\n",
            "Errors in epoch 57: 8019\n",
            "Updated Weights: [-1.         -0.03197561 -0.22531751  1.09182742]\n",
            "Errors in epoch 58: 8050\n",
            "Updated Weights: [-1.          0.54283807  0.05773892  2.3836066 ]\n",
            "Errors in epoch 59: 8123\n",
            "Updated Weights: [-2.          0.38312609 -0.94782755  2.33882065]\n",
            "Errors in epoch 60: 8058\n",
            "Updated Weights: [-2.          0.70246702  0.86924811  2.57299728]\n",
            "Errors in epoch 61: 8007\n",
            "Updated Weights: [-1.          0.73444263 -0.08390224  0.69201367]\n",
            "Errors in epoch 62: 8005\n",
            "Updated Weights: [-2.         -0.25555577  0.75049212  2.07894172]\n",
            "Errors in epoch 63: 8115\n",
            "Updated Weights: [-1.         -0.5429211   0.40092957  2.09992923]\n",
            "Errors in epoch 64: 7997\n",
            "Updated Weights: [-2.00000000e+00 -8.30286419e-05 -1.97592955e-01  1.99993090e+00]\n",
            "Errors in epoch 65: 8054\n",
            "Updated Weights: [-2.         -0.06395121  1.15470085  1.08987866]\n",
            "Errors in epoch 66: 8088\n",
            "Updated Weights: [0.         0.19160456 0.12358881 1.04808318]\n",
            "Errors in epoch 67: 8049\n",
            "Updated Weights: [-1.         -0.03197561  1.10480434  2.05709204]\n",
            "Errors in epoch 68: 8092\n",
            "Updated Weights: [-1.          0.54283807  0.18168168  2.1322951 ]\n",
            "Errors in epoch 69: 8010\n",
            "Updated Weights: [-1.         -0.60678928  0.43376513  2.13610285]\n",
            "Errors in epoch 70: 8026\n",
            "Updated Weights: [-1.          0.15962895 -0.26610425  1.23747386]\n",
            "Errors in epoch 71: 8060\n",
            "Updated Weights: [-1.          0.54283807 -0.28902179  1.78674179]\n",
            "Errors in epoch 72: 8097\n",
            "Updated Weights: [-2.          0.06378516  0.87070845  1.74671553]\n",
            "Errors in epoch 73: 8117\n",
            "Updated Weights: [-1.          0.862179    0.69993547  2.56447472]\n",
            "Errors in epoch 74: 8074\n",
            "Updated Weights: [-1.          0.79831081 -0.39608516  1.1089623 ]\n",
            "Errors in epoch 75: 8082\n",
            "Updated Weights: [-1.          0.862179   -0.47331638  1.26127229]\n",
            "Errors in epoch 76: 8044\n",
            "Updated Weights: [-1.         -0.15971198 -0.82914576  2.40264535]\n",
            "Errors in epoch 77: 8064\n",
            "Updated Weights: [-1.          0.54283807 -0.26752841  1.72010617]\n",
            "Errors in epoch 78: 8126\n",
            "Updated Weights: [-1.          0.67057444 -0.12734157  1.73819298]\n",
            "Errors in epoch 79: 8060\n",
            "Updated Weights: [-1.         -0.15971198 -0.81274541  2.43310735]\n",
            "Errors in epoch 80: 8050\n",
            "Updated Weights: [-1.         -0.28744835 -0.65051952  1.99236029]\n",
            "Errors in epoch 81: 8038\n",
            "Updated Weights: [-1.          0.92604719 -0.4846715   1.21081961]\n",
            "Errors in epoch 82: 8073\n",
            "Updated Weights: [-2.          0.38312609 -0.9451075   2.34643615]\n",
            "Errors in epoch 83: 8080\n",
            "Updated Weights: [-2.         -0.1278194   1.02392101  2.14652928]\n",
            "Errors in epoch 84: 8071\n",
            "Updated Weights: [-1.          0.54283807 -0.64502197  2.12848735]\n",
            "Errors in epoch 85: 8066\n",
            "Updated Weights: [-1.         -0.35131654 -0.10015404  2.27222991]\n",
            "Errors in epoch 86: 8051\n",
            "Updated Weights: [-2.          0.12765334  0.10757221  2.19222228]\n",
            "Errors in epoch 87: 8054\n",
            "Updated Weights: [-2.         -0.19168759  0.9553356   2.32834934]\n",
            "Errors in epoch 88: 8079\n",
            "Updated Weights: [-1.          0.4151017  -0.62306835  2.10468891]\n",
            "Errors in epoch 89: 8114\n",
            "Updated Weights: [-1.          0.67057444 -0.33080593  0.71486017]\n",
            "Errors in epoch 90: 8074\n",
            "Updated Weights: [-1.          0.67057444  1.06509218  2.29031672]\n",
            "Errors in epoch 91: 8027\n",
            "Updated Weights: [-2.          0.19152153  0.70182406  1.43828778]\n",
            "Errors in epoch 92: 8093\n",
            "Updated Weights: [-1.          0.47896988 -0.62223598  2.19417104]\n",
            "Errors in epoch 93: 8110\n",
            "Updated Weights: [-1.          0.98991537 -0.34933997  2.39407791]\n",
            "Errors in epoch 94: 8053\n",
            "Updated Weights: [-2.          0.19152153  1.01385713  1.60487685]\n",
            "Errors in epoch 95: 8011\n",
            "Updated Weights: [-1.          0.92604719 -0.2475576   2.3607601 ]\n",
            "Errors in epoch 96: 8014\n",
            "Updated Weights: [-1.         -0.5429211  -0.62252064  1.58493104]\n",
            "Errors in epoch 97: 7997\n",
            "Updated Weights: [-2.          0.19152153  0.75125756  1.50397147]\n",
            "Errors in epoch 98: 8037\n",
            "Updated Weights: [-1.          0.79831081  0.15208512  1.40691873]\n",
            "Errors in epoch 99: 8014\n",
            "Updated Weights: [-1.          0.79831081 -0.43614351  1.89811848]\n",
            "\n",
            "Accuracy: 0.5924\n",
            "\n",
            "F1 Score: 0.48036715961244264\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "# you can change the iteration number if you want\n",
        "perceptron = Perceptron(X_train, n_iter=100)\n",
        "### END CODE HERE ###\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred = perceptron.predict(X_val)\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"\\nF1 Score:\", f1_score(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqU2bONVAAg-"
      },
      "source": [
        "## Save the test result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "DU2sljAh_wfG"
      },
      "outputs": [],
      "source": [
        "y_pred = perceptron.predict(X_test)\n",
        "with open(output_path_part1, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['id', 'diabetes_mellitus'])\n",
        "  for i in range(len(y_pred)):\n",
        "    writer.writerow([i, y_pred[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emvl27OWCLFD"
      },
      "source": [
        "# Part 2 - LDA\n",
        "\n",
        "In this part, you'll be implementing key components of Linear Discriminant Analysis (LDA).\n",
        "\n",
        "Here's what you need to focus on:\n",
        "\n",
        ">**The fisher_discriminant function**\n",
        ">* Compute the within-class scatter matrix (S_W)\n",
        ">> $S_{W} = \\sum_{n \\in C_{1}}(X_{n}-m_{1})(X_{n}-m_{1})^{T} + \\sum_{n \\in C_{2}}(X_{n}-m_{2})(X_{n}-m_{2})^{T}$\n",
        ">* Compute the between-class scatter matrix (S_B)\n",
        ">> $S_{B} = (m_{2}-m_{1})(m_{2}-m_{1})^{T}$\n",
        ">* Calculate the discriminant vector (w) using S_W and the class means\n",
        ">> $w = S_{W}^{-1}(m_{2}-m_{1})$\n",
        ">>\n",
        ">> note that we define **$m_{1}$=class 0, $m_{2}$=class 1**\n",
        ">* Normalize the discriminant vector\n",
        ">\n",
        ">>Hints:\n",
        ">>* Remember to invert S_W using np.linalg.inv\n",
        ">>* Normalize the final vector using np.linalg.norm\n",
        ">\n",
        ">**The boundary_calculation function**\n",
        ">* This function calculates the decision boundary in the LDA-transformed space\n",
        ">* Calculate the mean of each class in the transformed space\n",
        ">* Compute the decision boundary as the average of these means\n",
        ">\n",
        ">>Hints:\n",
        ">>* Use numpy's mean function (np.mean) with boolean indexing to separate classes\n",
        ">\n",
        ">**The lda_classifier function**\n",
        ">* This function ties everything together to perform LDA classification.\n",
        ">* Project the training and test data onto the LDA space\n",
        ">* Calculate the decision boundary\n",
        ">* Classify the test data based on this boundary\n",
        ">\n",
        ">>Hints:\n",
        ">>* Use the dot product (.dot()) to project data onto the discriminant vector\n",
        ">>* Implement the classification logic using a simple if-else statement\n",
        "\n",
        "**Please save the prediction result in a csv file lab3_part2.csv and upload to Kaggle**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYkZDHgPH9LN"
      },
      "source": [
        "## Define Fisher's linear discriminant function\n",
        "\n",
        "Reference:\n",
        "\n",
        "slides L5 p.22-23\n",
        "\n",
        "https://sthalles.github.io/fisher-linear-discriminant/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HF6qQtvsGbCC"
      },
      "outputs": [],
      "source": [
        "def fisher_discriminant(X, y):\n",
        "  classes = np.unique(y)\n",
        "  # Compute mean vectors for each class\n",
        "  mean_vectors = [np.mean(X[y == cls], axis=0) for cls in classes]\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Compute within-class scatter matrix\n",
        "  S_W = np.zeros((X.shape[1], X.shape[1]))\n",
        "  for cls, mean_vec in zip(classes, mean_vectors):\n",
        "    S_W += (X[y == cls] - mean_vec).T @ (X[y == cls] - mean_vec)\n",
        "\n",
        "  # Compute between-class scatter matrix\n",
        "  mean_diff = (mean_vectors[1] - mean_vectors[0]).reshape(-1, 1)\n",
        "  S_B = mean_diff @ mean_diff.T\n",
        "\n",
        "  # Compute the discriminant vector\n",
        "  w = np.linalg.inv(S_W) @ (mean_vectors[1] - mean_vectors[0])\n",
        "\n",
        "  # Normalize the discriminant vector\n",
        "  w = w / np.linalg.norm(w)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNbzjNSXMIpZ"
      },
      "source": [
        "### Use the example data to test the weight caculation\n",
        "Expected output:\n",
        "> [ 0.37541286  0.64630924 -0.66434144]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "I-ldjqUTH3TC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.37541286  0.64630924 -0.66434144]\n"
          ]
        }
      ],
      "source": [
        "# Get the discriminant\n",
        "# X_exp has been standardized\n",
        "W_exp = fisher_discriminant(X_exp, y_exp)\n",
        "print(W_exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbL6lBm-TYVs"
      },
      "source": [
        "## Implement a classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OPMQBnAuT65h"
      },
      "outputs": [],
      "source": [
        "def boundary_calculation(X_train_lda, y_train):\n",
        "  # Calculate the means and variances of the classes in the projected space\n",
        "  ### START CODE HERE ###\n",
        "  mean_class_0 = np.mean(X_train_lda[y_train == 0])\n",
        "  mean_class_1 = np.mean(X_train_lda[y_train == 1])\n",
        "\n",
        "  decision_boundary = (mean_class_0 + mean_class_1) / 2\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return decision_boundary\n",
        "\n",
        "def lda_classifier(X_train, y_train, X_test):\n",
        "\n",
        "  W = fisher_discriminant(X_train, y_train)\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Project onto the first discriminant\n",
        "  X_train_lda = X_train.dot(W)\n",
        "  X_test_lda = X_test.dot(W)\n",
        "\n",
        "  decision_boundary = boundary_calculation(X_train_lda, y_train)\n",
        "\n",
        "  y_pred = np.where(X_test_lda >= decision_boundary, 1, 0)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V2uoF0xewxF"
      },
      "source": [
        "### Use the example data to test the boundary calculation\n",
        "Expected output:\n",
        "> 0.5028438197095305"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zwGyWfTze7Dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5028438197095305\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ###\n",
        "w = fisher_discriminant(X_exp, y_exp)\n",
        "X_exp_lda = X_exp.dot(w)\n",
        "### END CODE HERE ###\n",
        "print(boundary_calculation(X_exp_lda, y_exp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyWhnsiUVLvy"
      },
      "source": [
        "## Train and validate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "45jpURFTVK-o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 68.88%\n",
            "\n",
            "F1 Score: 0.6847649918962723\n"
          ]
        }
      ],
      "source": [
        "# Classify the projected test data\n",
        "y_pred = lda_classifier(X_train, y_train, X_val)\n",
        "# print(type(y_pred))\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nF1 Score:\", f1_score(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv_daOx5WRho"
      },
      "source": [
        "## Save the test result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "YMLVi64KWOrL"
      },
      "outputs": [],
      "source": [
        "y_pred = lda_classifier(X_train, y_train, X_test)\n",
        "with open(output_path_part2, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['id', 'diabetes_mellitus'])\n",
        "  for i in range(len(y_pred)):\n",
        "    writer.writerow([i, y_pred[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1hhfBbMCs9g"
      },
      "source": [
        "# Part 3 - LDA with MAP\n",
        "\n",
        "In this part, you're implementing a Linear Discriminant Analysis (LDA) classifier **using** Gaussian distributions and Maximum A Posterior (MAP) estimation.\n",
        "\n",
        ">1. **Linear Discriminant Analysis (LDA):**\n",
        ">LDA is a method that finds a linear combination of features that best separates two or more classes. It assumes that the classes are normally distributed with equal covariance matrices.\n",
        ">\n",
        ">2. **Gaussian Density Function:**\n",
        ">The Gaussian (or normal) distribution is defined by its probability density function:\n",
        ">\n",
        ">>$f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$\n",
        ">>\n",
        ">>Where μ is the mean and σ² is the variance.\n",
        ">\n",
        ">3. **Maximum A Posteriori (MAP) Estimation:**\n",
        ">MAP estimation seeks to find the most probable class given the observed data. It combines the likelihood of the data given the class (from the Gaussian density function) with the prior probability of the class.\n",
        "\n",
        "Connecting LDA, Gaussian Distributions, and MAP:\n",
        ">Step 1: LDA projects the data onto a lower-dimensional space that maximizes class separability (the **lda_classifier_map** function in the code)\n",
        ">\n",
        ">Step 2: After projection, we assume each class follows a Gaussian distribution in this new space. Computes the means, variances, and priors of each class in the LDA-projected space. (the **mean_variance_prior** function in the code)\n",
        ">\n",
        ">Step 3: Implement the Gaussian density function. (the **likelihood** function in the code)\n",
        ">\n",
        ">Step 4: Use MAP estimation (the **lda_classifier_map** function in the code)\n",
        ">>* For each test point, calculate its likelihood of belonging to each class using the likelihood function (which you've already implemented).\n",
        ">>* Multiply these likelihoods by the class priors to get quantities proportional to the posterior probabilities.\n",
        ">>* Predict based on the highest posterior probability.\n",
        "\n",
        "**Please save the prediction result in a csv file lab3_part3.csv and upload to Kaggle**\n",
        "\n",
        "Reference: https://sthalles.github.io/fisher-linear-discriminant/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8iEGC07ckYz"
      },
      "source": [
        "## Implement a classifier\n",
        "\n",
        "Reference: [Linear Discriminant Analysis](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-lda%E5%88%86%E9%A1%9E%E6%BC%94%E7%AE%97%E6%B3%95-14622f29e4dc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "B5wgzjcIcrqq"
      },
      "outputs": [],
      "source": [
        "# Define a function to compute the likelihood\n",
        "def mean_variance_prior(X_train_lda, y_train):\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Calculate the means and variances of the classes in the projected space\n",
        "  mean_class_0 = np.mean(X_train_lda[y_train == 0])\n",
        "  mean_class_1 = np.mean(X_train_lda[y_train == 1])\n",
        "  variance_class_0 = np.var(X_train_lda[y_train == 0])\n",
        "  variance_class_1 = np.var(X_train_lda[y_train == 1])\n",
        "\n",
        "  # Calculate the prior probabilities\n",
        "  prior_class_0 = np.mean(y_train == 0)\n",
        "  prior_class_1 = np.mean(y_train == 1)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return mean_class_0, variance_class_0, prior_class_0, mean_class_1, variance_class_1, prior_class_1\n",
        "\n",
        "def likelihood(mean, variance, x): # implement the Gaussian density distribution function\n",
        "  ### START CODE HERE ###\n",
        "  likelihood = (1 / np.sqrt(2 * np.pi * variance)) * np.exp(-((x - mean) ** 2) / (2 * variance))\n",
        "  ### END CODE HERE ###\n",
        "  return likelihood\n",
        "\n",
        "def lda_classifier_map(X_train, y_train, X_test):\n",
        "\n",
        "  W = fisher_discriminant(X_train, y_train)\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Project onto the first discriminant\n",
        "  X_train_lda = X_train.dot(W)\n",
        "  X_test_lda = X_test.dot(W)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  mean_class_0, variance_class_0, prior_class_0, mean_class_1, variance_class_1, prior_class_1 = mean_variance_prior(X_train_lda, y_train)\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Classify based on the maximum posterior probability\n",
        "  predictions = []\n",
        "  for x in X_test_lda:\n",
        "    likelihood_0 = likelihood(mean_class_0, variance_class_0, x)\n",
        "    posterior_0 = likelihood_0 * prior_class_0\n",
        "\n",
        "    likelihood_1 = likelihood(mean_class_1, variance_class_1, x)\n",
        "    posterior_1 = likelihood_1 * prior_class_1\n",
        "\n",
        "    predictions.append(1 if posterior_1 > posterior_0 else 0)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return np.array(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANsnJ280m_ep"
      },
      "source": [
        "### Use the example data to test the likelihood calculation\n",
        "If the differences between your output and expected output are only in last few decimal places, it's unlikely to affect the model's final results.\n",
        "\n",
        "Expected output:\n",
        ">means: 0.029797169482780564 0.9762686734207664\n",
        ">\n",
        ">variances: 0.36972519622481526 0.22878455291253338\n",
        ">\n",
        ">priors: 0.8421052631578947 0.15789473684210525\n",
        ">\n",
        ">likelihoods: 0.6560640840455648 0.11464725416998729"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ak9M6fWAnCBw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "means: 0.0297971694827806 0.9762686734207663\n",
            "variances: 0.3697251962248152 0.22878455291253327\n",
            "priors: 0.8421052631578947 0.15789473684210525\n",
            "likelihoods: 0.6560640840455648 0.11464725416998721\n"
          ]
        }
      ],
      "source": [
        "mean_class_0, variance_class_0, prior_class_0, mean_class_1, variance_class_1, prior_class_1 = mean_variance_prior(X_exp_lda[:19], y_exp[:19])\n",
        "print(\"means:\", mean_class_0, mean_class_1)\n",
        "print(\"variances:\", variance_class_0, variance_class_1)\n",
        "print(\"priors:\", prior_class_0, prior_class_1)\n",
        "print(\"likelihoods:\", likelihood(mean_class_0, variance_class_0, X_exp_lda[19]), likelihood(mean_class_1, variance_class_1, X_exp_lda[19]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cD0iDKEcmIZ"
      },
      "source": [
        "## Train and validate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "TOBXCOrlcqil"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 68.82%\n",
            "\n",
            "F1 Score: 0.6875125275606334\n"
          ]
        }
      ],
      "source": [
        "# Classify the projected test data\n",
        "y_pred = lda_classifier_map(X_train, y_train, X_val)\n",
        "# print(type(y_pred))\n",
        "accuracy = np.mean(y_pred == y_val)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nF1 Score:\", f1_score(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfArDC9qhxms"
      },
      "source": [
        "## Save the test result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "HTb1pDJVhzOJ"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = lda_classifier_map(X_train, y_train, X_test)\n",
        "# Write the prediction to output csv\n",
        "with open(output_path_part3, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  writer.writerow(['id', 'diabetes_mellitus'])\n",
        "  for i in range(len(y_pred)):\n",
        "    writer.writerow([i, y_pred[i]])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
