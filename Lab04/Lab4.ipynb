{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQAqOS2xwiYG"
   },
   "source": [
    "Mount Google Drive (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "dA4DURCWwvWU"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IagZMs0_qjdL"
   },
   "source": [
    "# **Lab 4 : Neural Network**\n",
    "\n",
    "In *lab 4*, you need to finish:\n",
    "\n",
    "1. Basic Part (65%):\n",
    "  Implement a deep neural network from scratch\n",
    "\n",
    "  > * Section 1: Neural network implementation\n",
    "    >> * Part 1: Linear layer\n",
    "    >> * Part 2: Activation function layer\n",
    "    >> * Part 3: Build model\n",
    "\n",
    "  > * Section 2: Loss function\n",
    "    >> * Part 1: Binary cross-entropy loss (BCE)\n",
    "    >> * Part 2: Categorical cross-entropy loss (CCE)\n",
    "    >> * Part 3: Mean square error (MSE)\n",
    "  > * Section 3: Training and prediction\n",
    "    >> * Part 1: Training function & batch function\n",
    "    >> * Part 2: Regression\n",
    "    >> * Part 3: Binary classification\n",
    "\n",
    "\n",
    "2. Advanced Part (30%): Multi class classification\n",
    "3. Report (5%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGFR00CQvoaH"
   },
   "source": [
    "## **Important  notice**\n",
    "\n",
    "* Please **do not** change the code outside this code bracket in the basic part.\n",
    "  ```\n",
    "  ### START CODE HERE ###\n",
    "  ...\n",
    "  ### END CODE HERE ###\n",
    "  ```\n",
    "\n",
    "* Please **do not** import any other packages in both basic and advanced part\n",
    "\n",
    "* Please **do not** change the random seed **np.random.seed(1)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BgcgLVV79Bm"
   },
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "fmTH9UkeqdYf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO31dEFx-C1y"
   },
   "source": [
    "### Common Notation\n",
    "  * $C$: number of classes\n",
    "  * $n$: number of samples\n",
    "  * $f^{[l]}$: the dimension of outputs in layer $l$, but $f^{[0]}$ is the input dimension\n",
    "  * $Z^{[l]} = A^{[l-1]}W^{[l]} + b^{[l]}$\n",
    "      * $Z^{[l]}$: the output of layer $l$ in the shape $(n, f^{[l]})$\n",
    "      * $A^{[l]}$: the activation of $Z^{[l]}$ in the shape $(n, f^{[l]})$, but $A^{[0]}$ is input $X$\n",
    "      * $W^{[l]}$: the weight in layer $l$ in the shape $(f^{[l-1]}, f^{[l]})$\n",
    "      * $b^{[l]}$: the bias in layer $l$ in the shape $(1, f^{[l]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wE5z0w8FQLK"
   },
   "source": [
    "# **Basic Part (65%)**\n",
    "In the Basic Part, you will implement a neural network framework capable of handling both regression, binary classification and multi-class classification tasks.\n",
    "\n",
    "**Note:**\n",
    "After implementing each class/function, test it with the provided input variables to verify its correctness. Save the results in the **outputs** dictionary. (The code for testing and saving results is already provided.)\n",
    "## Section 1: Neural network implementation\n",
    "* Part 1: Linear layer\n",
    "> * Step 1: Linear Initialize parameters\n",
    "> * Step 2: Linear forward\n",
    "> * Step 3: Linear backward\n",
    "> * Step 4: Linear update parameters\n",
    "* Part 2: Activation function layer\n",
    "> * Step 1: Activation forward\n",
    "> * Step 2: Activation backward\n",
    "* Part 3: Build model\n",
    "> * Step 1: Model Initialize parameters\n",
    "> * Step 2: Model forward\n",
    "> * Step 3: Model backward\n",
    "> * Step 4: Model update parameters\n",
    "\n",
    "## Section 2: Loss function\n",
    "* Part 1: Binary cross-entropy loss (BCE)\n",
    "* Part 2: Categorical cross-entropy loss (CCE)\n",
    "* Part 3: Mean square error (MSE)\n",
    "\n",
    "## Section 3: Training and prediction\n",
    "* Part 1: Training function & batch function\n",
    "* Part 2: Regression\n",
    "* Part 3: Binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w35ZkTwMc00G"
   },
   "source": [
    "## **Section 1: Neural network implementation(30%)**\n",
    "To implement a neural network, you need to complete 3 classes: **Dense**, **Activation**, and **Model**.\n",
    "The process of training a deep neural network is composed of 3 steps: *forward propagation*, *backward propagation*, and *update*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_krGKUNg_Ix"
   },
   "source": [
    "## Part 1: Linear layer (10%)\n",
    "Dense layer (fully-connected layer) performs linear transformation:\n",
    "\n",
    "$Z = AW + b$, where W is weight matrix and b is bias vector.\n",
    "\n",
    "> ### Step 1: Initialize parameters (0%)\n",
    " * You don't need to write this part.\n",
    " * W is randomly initialized using uniform distribution within $[\\text\\{-limit\\}, \\text\\{limit\\}]$, where $\\text\\{limit\\} = \\sqrt{\\frac{6}{\\text\\{fanin\\} + \\text\\{fanout\\}}}$ (fanin: number of input features, fanout: number of output features)\n",
    " * b is initialized to 0\n",
    "\n",
    "> ### Step 2: Linear forward (4%)\n",
    "* Compute Z using matrix multiplication and addition\n",
    "\n",
    "> ### Step 3: Linear backward (4%)\n",
    "* Use backpropagation to compute gradients of loss function with respect to parameters\n",
    "* For layer l: $Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$ (followed by activation)\n",
    "* Given $dZ^{[l]}$ (gradient of loss with respect to Z), we need to compute three gradients:\n",
    "  * $dW^{[l]}$: gradient of loss with respect to weights\n",
    "  * $db^{[l]}$: gradient of loss with respect to bias\n",
    "  * $dA^{[l-1]}$: gradient of loss with respect to previous layer output\n",
    "\n",
    "> Formulas:\n",
    "$$ dW^{[l]} = \\frac{1}{n} A^{[l-1] T} dZ^{[l]} $$\n",
    "$$ db^{[l]} = \\frac{1}{n} \\sum_{i = 1}^{n} dZ_i^{[l]} $$\n",
    "$$ dA^{[l-1]} = dZ^{[l]} W^{[l] T} $$\n",
    "\n",
    "> ### Step 4: Linear update parameters (2%)\n",
    "* Update parameters using gradient descent:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "x0KHo8w9yqbY"
   },
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, n_x, n_y, seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.seed = seed\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        self.n_x -- size of the input layer\n",
    "        self.n_y -- size of the output layer\n",
    "        self.parameters -- python dictionary containing your parameters:\n",
    "                           W -- weight matrix of shape (n_x, n_y)\n",
    "                           b -- bias vector of shape (1, n_y)\n",
    "        \"\"\"\n",
    "        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
    "        b = np.zeros((1, self.n_y))\n",
    "\n",
    "        assert(W.shape == (self.n_x, self.n_y))\n",
    "        assert(b.shape == (1, self.n_y))\n",
    "\n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "        Arguments:\n",
    "        A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n",
    "        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "\n",
    "        Returns:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n",
    "        \"\"\"\n",
    "\n",
    "        # GRADED FUNCTION: linear_forward\n",
    "        ### START CODE HERE ###\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = np.dot(A, W) + b\n",
    "        self.cache = (A, W, b)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "        Arguments:\n",
    "        dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n",
    "        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "        self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "\n",
    "        \"\"\"\n",
    "        A_prev, W, b = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        # GRADED FUNCTION: linear_backward\n",
    "        ### START CODE HERE ###\n",
    "        self.dW = np.dot(A_prev.T, dZ) / m  # dW = (A_prev^T * dZ) / m\n",
    "        self.db = np.sum(dZ, axis=0, keepdims=True) / m  # db = sum(dZ) / m\n",
    "        dA_prev = np.dot(dZ, W.T)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
    "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "\n",
    "        Arguments:\n",
    "        learning rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        # GRADED FUNCTION: linear_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        self.parameters[\"W\"] -= learning_rate * self.dW\n",
    "        self.parameters[\"b\"] -= learning_rate * self.db\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbnVsi6VJMXD"
   },
   "source": [
    "### Test your **Dense class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "7HNAWwmg8R7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[-0.20325375]\n",
      " [ 0.53968259]\n",
      " [-1.22446471]]\n",
      "b = [[0.]]\n",
      "Z = [[1.9]\n",
      " [2.2]\n",
      " [2.5]]\n",
      "dA_prev = [[3.5]\n",
      " [6. ]]\n",
      "dW = [[1.625 0.625]]\n",
      "db = [[2.   0.75]]\n",
      "W = [[0.5 2.5]]\n",
      "b = [[-1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameters\n",
    "dense = Dense(3, 1)\n",
    "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
    "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
    "\n",
    "# Linear forward\n",
    "A, W, b = np.array([[0., 1., 2.], [0.5, 1.5, 2.5], [1., 2., 3.]]), np.array([[0.1], [0.2], [0.3]]), np.array([[1.1]])\n",
    "dense = Dense(3, 1)\n",
    "dense.parameters = {\"W\": W, \"b\": b}\n",
    "Z = dense.forward(A)\n",
    "print(\"Z = \" + str(Z))\n",
    "\n",
    "A, W, b = np.array([[-0.80,-0.45,-1.11],[-1.65,-2.36,1.14],[-1.02,0.64,-0.86]]), np.array([[0.3], [0.3], [0.1]]), np.array([[-6.2]])\n",
    "dense = Dense(3, 1)\n",
    "dense.parameters = {\"W\": W, \"b\": b}\n",
    "Z = dense.forward(A)\n",
    "outputs[\"dense_forward\"] = (Z, dense.cache)\n",
    "\n",
    "# Linear backward\n",
    "dZ, linear_cache = np.array([[1.5, 0.5], [2.5, 1.]]), (np.array([[0.5], [1]]), np.array([[2., 1.0]]), np.array([[0.5, 1.]]))\n",
    "dense = Dense(1, 2)\n",
    "dense.cache = linear_cache\n",
    "dA_prev = dense.backward(dZ)\n",
    "print (\"dA_prev = \" + str(dA_prev))\n",
    "print (\"dW = \" + str(dense.dW))\n",
    "print (\"db = \" + str(dense.db))\n",
    "\n",
    "dZ, linear_cache = np.array([[0.52,0.34],[0.76,0.89]]), (np.array([[0.42], [0.68]]), np.array([[0.35, 0.89]]), np.array([[0.12, 0.76]]))\n",
    "dense = Dense(1, 2)\n",
    "dense.cache = linear_cache\n",
    "dA_prev = dense.backward(dZ)\n",
    "outputs[\"dense_backward\"] = (dA_prev, dense.dW, dense.db)\n",
    "\n",
    "# Linear update parameters\n",
    "np.random.seed(1)\n",
    "dense = Dense(1, 2)\n",
    "dense.parameters = {\"W\": np.array([[1.0, 2.0]]), \"b\": np.array([[0.5, 0.5]])}\n",
    "dense.dW = np.array([[0.5, -0.5]])\n",
    "dense.db = np.array([[1.5, -1.5]])\n",
    "dense.update(1.0)\n",
    "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
    "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "dense = Dense(3, 4)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(1,4)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(1,4)}\n",
    "dense.parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "dense.dW = grads[\"dW1\"]\n",
    "dense.db = grads[\"db1\"]\n",
    "dense.update(0.1)\n",
    "outputs[\"dense_update_parameters\"] = {\"W\": dense.parameters[\"W\"], \"b\": dense.parameters[\"b\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtPtH0j3BFN7"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>W: </td>\n",
    "    <td>[[-0.20325375]  [0.53968259 [-1.22446471]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b: </td>\n",
    "    <td>[[0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Z: </td>\n",
    "    <td>[[1.9] [2.2] [2.5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev: </td>\n",
    "    <td>[[3.5] [6.0]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW: </td>\n",
    "    <td>[[1.625 0.625]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db: </td>\n",
    "    <td>[[2.0 0.75]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W: </td>\n",
    "    <td>[[0.5 2.5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b: </td>\n",
    "    <td>[[-1.  2.]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r5m2W3aXh_A"
   },
   "source": [
    "## Part 2: Activation function layer (10%)\n",
    "\n",
    "Implement forward and backward propagation for activation function layers, including Sigmoid, Softmax, and ReLU.\n",
    "\n",
    "> ### Step 1: Forward Propagation (5%)\n",
    " Implement the following activation functions:\n",
    ">> #### a) Sigmoid\n",
    "- Use the numerically stable version to prevent exponential overflow:\n",
    "  $$\\sigma(Z) = \\begin{cases}\n",
    "    \\frac{1}{1+e^{-Z}},& \\text{if } Z \\geq 0\\\\\n",
    "    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n",
    "  \\end{cases}$$\n",
    "\n",
    ">> #### b) ReLU\n",
    "- Simple implementation:\n",
    "  $$RELU(Z) = \\max(Z, 0)$$\n",
    "\n",
    ">> #### c) Softmax\n",
    "- Implement using the numerically stable version:\n",
    "  $$\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{C} e^{Z_j-b}}$$\n",
    "  where $b = \\max_{j=1}^{C} Z_j$\n",
    "\n",
    ">> #### d) Linear\n",
    "- You don't need to implement this part\n",
    "\n",
    "> ### Requirements\n",
    "- Each function should return:\n",
    "  1. Activation value \"a\"\n",
    "  2. Cache containing \"z\" for backward propagation\n",
    "\n",
    "> ### Step 2: Backward Propagation (5%)\n",
    "Implement backward functions for:\n",
    "- Sigmoid\n",
    "- ReLU\n",
    "- Softmax\n",
    "- linear\n",
    "\n",
    "> ### General Form\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "where $g(.)$ is the activation function\n",
    "\n",
    "> ### Specific Implementations\n",
    "\n",
    ">> #### a) Sigmoid Backward\n",
    "$$\\sigma'(Z^{[l]}) = \\sigma(Z^{[l]}) (1 - \\sigma(Z^{[l]}))$$\n",
    "Use numerically stable sigmoid\n",
    "\n",
    ">> #### b) ReLU Backward\n",
    "$$g'(Z^{[l]}) = \\begin{cases}\n",
    "    1,& \\text{if } Z^{[l]} > 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    ">> #### c) Softmax Backward\n",
    "For the special case of Softmax combined with Categorical Cross-Entropy loss:\n",
    "$$dZ^{[l]} = s - y$$\n",
    "where $s$ is softmax output, $y$ is true label (one-hot vector)\n",
    "\n",
    "Note: This is a simplified form specific to Softmax + CCE loss combination.\n",
    "\n",
    ">> #### d) linear Backward\n",
    "You don't need to implement this part\n",
    "\n",
    "> ### Note\n",
    "For softmax, use the normalized exponential function to prevent overflow, but use the simplified gradient equation for backwards propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "Nnuv8MmebMgg"
   },
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    def __init__(self, activation_function, loss_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implements the sigmoid activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of sigmoid(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: sigmoid_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the RELU function in numpy\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "            Returns:\n",
    "            A -- output of relu(z), same shape as Z\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: relu_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.maximum(0, Z)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert(A.shape == Z.shape)\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implements the softmax activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- np.array with shape (n, C)\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of softmax(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_forward\n",
    "            ### START CODE HERE ###\n",
    "            exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "            A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"linear\":\n",
    "            \"\"\"\n",
    "            Linear activation (returns Z directly).\n",
    "            \"\"\"\n",
    "            self.cache = Z.copy()\n",
    "            return Z\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "\n",
    "    def backward(self, dA=None, Y=None):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single SIGMOID unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: sigmoid_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            s = 1 / (1 + np.exp(-Z))\n",
    "            dZ = dA * s * (1 - s)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single RELU unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: relu_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            dZ = np.array(dA, copy=True)\n",
    "            dZ[Z <= 0] = 0\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
    "            Arguments:\n",
    "            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the cost with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            s = np.exp(Z - np.max(Z, axis=1, keepdims=True)) / np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "            dZ = s - Y\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"linear\":\n",
    "            \"\"\"\n",
    "            Backward propagation for linear activation.\n",
    "            \"\"\"\n",
    "            return dA\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDYVMMS2ecCx"
   },
   "source": [
    "### Test your **Activation class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "gBuRAoeUC5jV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: A = [[0.00669285]\n",
      " [0.26894142]\n",
      " [0.5       ]\n",
      " [0.73105858]\n",
      " [0.99330715]]\n",
      "ReLU: A = [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [5]]\n",
      "Softmax: A = \n",
      "[[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.0320586  0.08714432 0.23688282 0.64391426]]\n",
      "Linear: A = \n",
      "[[ 1  2  3  4]\n",
      " [ 0  1  0  0]\n",
      " [-2 -1  0  1]]\n",
      "Sigmoid: dZ = [[-0.5       ]\n",
      " [-0.26935835]\n",
      " [-0.11969269]\n",
      " [-0.5       ]\n",
      " [-0.73139639]]\n",
      "ReLU: dZ = [[ 0.    1.7 ]\n",
      " [ 0.    0.  ]\n",
      " [-1.14  3.72]]\n",
      "Softmax: dZ = [[-0.96488097  0.70538451  0.25949646]\n",
      " [ 0.09003057 -0.75527153  0.66524096]\n",
      " [ 0.01766842  0.01766842 -0.03533684]]\n",
      "Linear: dZ = \n",
      "[[ 1.2 -0.5  0.8 -0.3]\n",
      " [ 0.4  0.6 -0.9  0.2]\n",
      " [-0.1  0.5 -0.7  0.9]]\n"
     ]
    }
   ],
   "source": [
    "# Activation forward\n",
    "Z = np.array([[-5], [-1], [0], [1], [5]])\n",
    "\n",
    "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
    "A = sigmoid.forward(Z)\n",
    "print(\"Sigmoid: A = \" + str(A))\n",
    "A = sigmoid.forward(np.array([[0.23], [-0.67], [0.45], [0.89], [-0.10]]))\n",
    "outputs[\"sigmoid\"] = (A, sigmoid.cache)\n",
    "\n",
    "relu = Activation(\"relu\", 'cross_entropy')\n",
    "A = relu.forward(Z)\n",
    "print(\"ReLU: A = \" + str(A))\n",
    "A = relu.forward(np.array([[-0.34], [-0.76], [0.21], [-0.98], [0.54]]))\n",
    "outputs[\"relu\"] = (A, relu.cache)\n",
    "\n",
    "Z = np.array([[1, 2, 3, 4],[0, 1, 0, 0],[-2, -1, 0, 1]])\n",
    "softmax = Activation(\"softmax\", 'cross_entropy')\n",
    "A = softmax.forward(Z)\n",
    "print(\"Softmax: A = \\n\" + str(A))\n",
    "A = softmax.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
    "outputs[\"softmax\"] = (A, softmax.cache)\n",
    "\n",
    "linear = Activation(\"linear\", 'mse')\n",
    "A = linear.forward(Z)\n",
    "print(\"Linear: A = \\n\" + str(A))\n",
    "A = linear.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
    "outputs[\"linear\"] = (A, Z)  # For linear activation, cache is just Z\n",
    "\n",
    "# Activation backward\n",
    "dA, cache = np.array([[-2], [-1.37], [-1.14], [-2], [-3.72]]), np.array([[0], [1], [2], [0], [1]])\n",
    "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
    "sigmoid.cache = cache\n",
    "dZ = sigmoid.backward(dA=dA)\n",
    "print(\"Sigmoid: dZ = \"+ str(dZ))\n",
    "dA, cache = np.array([[9.73], [-7.56], [8.34], [-4.12], [6.89]]), np.array([[-5.45], [3.68], [-2.32], [4.51], [-9.27]])\n",
    "sigmoid.cache = cache\n",
    "outputs[\"sigmoid_backward\"] = sigmoid.backward(dA=dA)\n",
    "\n",
    "relu = Activation(\"relu\", 'cross_entropy')\n",
    "dA, cache = np.array([[-2., 1.7 ], [-1.37, 2.], [-1.14, 3.72]]), np.array([[-2, 1], [-1, 0], [2, 1]])\n",
    "relu.cache = cache\n",
    "dZ = relu.backward(dA=dA)\n",
    "print(\"ReLU: dZ = \"+ str(dZ))\n",
    "dA, cache = np.array([[7.24, -3.58], [8.93, 6.45], [-2.11, 9.87]]), np.array([[-4.76, 5.34], [1.98, -7.22], [3.67, -8.56]])\n",
    "relu.cache = cache\n",
    "outputs[\"relu_backward\"] = relu.backward(dA=dA)\n",
    "\n",
    "Y, cache = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]), np.array([[-2, 1, 0],[-1, 0, 1],[-2, -2, 2]])\n",
    "softmax = Activation(\"softmax\", 'cross_entropy')\n",
    "softmax.cache = cache\n",
    "dZ = softmax.backward(Y=Y)\n",
    "print(\"Softmax: dZ = \" + str(dZ))\n",
    "Y, cache = np.array([[0, 1, 0], [0, 1, 0], [1, 0, 0]]), np.array([[-9.45, 7.32, 3.58], [5.61, -8.27, 6.49], [1.23, -4.56, 7.84]])\n",
    "softmax.cache = cache\n",
    "outputs[\"softmax_backward\"] = softmax.backward(Y=Y)\n",
    "\n",
    "linear = Activation(\"linear\", 'mse')\n",
    "dA = np.array([[1.2, -0.5, 0.8, -0.3], [0.4, 0.6, -0.9, 0.2], [-0.1, 0.5, -0.7, 0.9]])\n",
    "dZ = linear.backward(dA=dA)\n",
    "print(\"Linear: dZ = \\n\" + str(dZ))\n",
    "outputs[\"linear_backward\"] = dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyyX_xxdEmNp"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Sigmoid: A</td>\n",
    "    <td>[[0.00669285] [0.26894142] [0.5] [0.73105858] [0.99330715]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReLU: A</td>\n",
    "    <td>[[0] [0] [0] [1] [5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Softmax: A</td>\n",
    "    <td>\n",
    "      [[0.0320586 0.08714432 0.23688282 0.64391426]\n",
    "       [0.1748777 0.47536689 0.1748777 0.1748777]\n",
    "       [0.0320586 0.08714432 0.23688282 0.64391426]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Linear: A</td>\n",
    "    <td>\n",
    "      [[1 2 3 4]\n",
    "       [0 1 0 0]\n",
    "       [-2 -1 0 1]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Sigmoid) dZ</td>\n",
    "    <td>[[-0.5] [-0.26935835] [-0.11969269] [-0.5] [-0.73139639]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with ReLU) dZ</td>\n",
    "    <td>[[0 1.7] [0 0] [-1.14 3.72]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Softmax) dZ</td>\n",
    "    <td>\n",
    "      [[-0.96488097 0.70538451 0.25949646]\n",
    "       [0.09003057 -0.75527153 0.66524096]\n",
    "       [0.01766842 0.01766842 -0.03533684]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Linear) dZ</td>\n",
    "    <td>\n",
    "      [[1.2 -0.5 0.8 -0.3]\n",
    "       [0.4 0.6 -0.9 0.2]\n",
    "       [-0.1 0.5 -0.7 0.9]]\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vcTYp_yoPu"
   },
   "source": [
    "## Part 3: Model (10%)\n",
    "\n",
    "Use the functions that you had previously written to implement the complete neural network model, including initialization, forward propagation, backward propagation, and parameter updates.\n",
    "\n",
    "> ### Step 1: Model Initialization (0%)\n",
    "Initialize the model by creating linear and activation function layers.\n",
    "\n",
    ">> #### Requirements:\n",
    "- Store linear layers in a list called `linear`\n",
    "- Store activation function layers in a list called `activation`\n",
    "- Use iteration number as seed for each Dense layer initialization\n",
    "\n",
    ">> #### Note:\n",
    "A linear-activation pair counts as a single layer in the neural network.\n",
    "\n",
    "> ### Step 2: Forward Propagation (4%)\n",
    "Implement the model's forward pass by calling each layer's forward function sequentially.\n",
    "\n",
    ">> #### Process:\n",
    "1. For layers 1 to N-1: [LINEAR -> ACTIVATION]\n",
    "2. Final layer: LINEAR -> SIGMOID (binary) or SOFTMAX (multi-class)\n",
    "\n",
    ">> #### Note:\n",
    "For binary classification, use one output node with sigmoid activation. For K-class classification, use K output nodes with softmax activation.\n",
    "\n",
    "> ### Step 3: Backward Propagation (4%)\n",
    "Implement the model's backward pass by calling each layer's backward function in reverse order.\n",
    "\n",
    ">> #### Process:\n",
    "1. Initialize backpropagation:\n",
    "   - Regression:\n",
    "     $$dAL = AL - Y$$\n",
    "   - Binary classification:\n",
    "     $$dAL = - (\\frac{Y}{AL + \\epsilon} - \\frac{1 - Y}{1 - AL + \\epsilon})$$\n",
    "     where $\\epsilon = 10^{-5}$ to prevent division by zero\n",
    "   - Multi-class classification:\n",
    "     Use `softmax_backward` function\n",
    "2. Backpropagate through layers L to 1\n",
    "\n",
    ">> #### Note:\n",
    "Use cached values from the forward pass in each layer's backward function.\n",
    "\n",
    "> ### Step 4: Parameter Update (2%)\n",
    "Update model parameters using gradient descent.\n",
    "\n",
    ">> #### Update Rule:\n",
    "For each layer $l = 1, 2, ..., L$:\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}$$\n",
    "$$b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$$\n",
    "where $\\alpha$ is the learning rate\n",
    "\n",
    "This revised structure provides a clear, step-by-step breakdown of the model implementation process, mirroring the format used in Part 2. It covers all the essential components while maintaining a concise and logical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "0JGMzfIDCSVz"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, units, activation_functions, loss_function):\n",
    "        self.units = units\n",
    "        self.activation_functions = activation_functions\n",
    "        self.loss_function = loss_function\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize layers of the neural network\n",
    "\n",
    "        Arguments:\n",
    "            self.units -- array defining network structure (e.g., [4,4,1]):\n",
    "                - Input layer: 4 nodes\n",
    "                - Hidden layer: 4 nodes\n",
    "                - Output layer: 1 node\n",
    "            self.activation_functions -- activation function for each layer (e.g., [\"relu\",\"sigmoid\"]):\n",
    "                - First layer uses ReLU\n",
    "                - Second layer uses Sigmoid\n",
    "            self.loss_function -- loss function type: \"cross_entropy\" or \"mse\"\n",
    "        \"\"\"\n",
    "        self.linear = []        # Store all Dense layers (weights & biases)\n",
    "        self.activation = []    # Store all activation function layers\n",
    "\n",
    "        for i in range(len(self.units)-1):\n",
    "            dense = Dense(self.units[i], self.units[i+1], i)\n",
    "            self.linear.append(dense)\n",
    "\n",
    "        for i in range(len(self.activation_functions)):\n",
    "            self.activation.append(Activation(self.activation_functions[i], self.loss_function))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network\n",
    "\n",
    "        Arguments:\n",
    "        X -- input data: shape (n, f)\n",
    "        Returns:\n",
    "        A -- model output:\n",
    "            - For binary classification: probability (0-1)\n",
    "            - For multi-class: probability distribution across classes\n",
    "            - For regression: predicted values\n",
    "        \"\"\"\n",
    "        A = X\n",
    "\n",
    "        # GRADED FUNCTION: model_forward\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(len(self.linear)):\n",
    "            Z = self.linear[i].forward(A)  # Forward pass through Dense layer\n",
    "            A = self.activation[i].forward(Z)  # Activation function on output\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients\n",
    "\n",
    "        Arguments:\n",
    "            AL -- model output from forward propagation:\n",
    "                - For binary: probability (n,1)\n",
    "                - For multi-class: probabilities (n,C)\n",
    "            Y -- true labels:\n",
    "                - For binary: 0/1 labels (n,1)\n",
    "                - For multi-class: one-hot vectors (n,C)\n",
    "                - For regression: true values (n,1)\n",
    "\n",
    "        Returns:\n",
    "            dA_prev -- gradients for previous layer's activation\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "        C = Y.shape[1]\n",
    "\n",
    "        # assertions\n",
    "        warning = 'Warning: only the following 3 combinations are allowed! \\n \\\n",
    "                    1. binary classification: sigmoid + cross_entropy \\n \\\n",
    "                    2. multi-class classification: softmax + cross_entropy \\n \\\n",
    "                    3. regression: linear + mse'\n",
    "        assert self.loss_function in [\"cross_entropy\", \"mse\"], \"you're using undefined loss function!\"\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "            if Y.shape[1] == 1:  # binary classification\n",
    "                assert self.activation_functions[-1] == 'sigmoid', warning\n",
    "            else:  # multi-class classification\n",
    "                assert self.activation_functions[-1] == 'softmax', warning\n",
    "                assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
    "        elif self.loss_function == \"mse\":\n",
    "            assert self.activation_functions[-1] == 'linear', warning\n",
    "            assert self.units[-1] == Y.shape[1], \"output dimension mismatch for regression!\"\n",
    "\n",
    "        # GRADED FUNCTION: model_backward\n",
    "        ### START CODE HERE ###\n",
    "        if self.activation_functions[-1] == \"linear\":\n",
    "            # Initializing the backpropagation\n",
    "            dAL = AL - Y  # MSE loss derivative\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dZ = dAL  # Linear activation, dZ = dAL\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"sigmoid\":\n",
    "            # Initializing the backpropagation\n",
    "            eps = 1e-5\n",
    "            dAL = -(Y / (AL + eps) - (1 - Y) / (1 - AL + eps))  # Cross-entropy derivative for binary classification\n",
    "            \n",
    "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dZ = self.activation[-1].backward(dAL)  # Backward pass through sigmoid activation\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"softmax\":\n",
    "            # Initializing the backpropagation\n",
    "            dZ = self.activation[-1].backward(Y=Y)  # Softmax + cross-entropy loss backward\n",
    "\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
    "        for i in reversed(range(L - 1)):  # Loop through the rest of the layers\n",
    "            dA_prev = self.activation[i].backward(dA_prev)  # Activation backward\n",
    "            dA_prev = self.linear[i].backward(dA_prev)  # Linear layer backward\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        learning_rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "\n",
    "        # GRADED FUNCTION: model_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(L):\n",
    "            self.linear[i].update(learning_rate)  # Update parameters for each Dense layer\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxQtZMmA1SNc"
   },
   "source": [
    "### Test your **Model class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "EGY7_1bjcm-c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:  [[ 0.09762701  0.08976637 -0.12482558]\n",
      " [ 0.43037873 -0.1526904   0.783546  ]\n",
      " [ 0.20552675  0.29178823  0.92732552]] \n",
      "b1:  [[0. 0. 0.]]\n",
      "W2:  [[-0.20325375]\n",
      " [ 0.53968259]\n",
      " [-1.22446471]] \n",
      "b2:  [[0.]]\n",
      "With sigmoid: A = [[0.64565631]\n",
      " [0.20915937]\n",
      " [0.77902611]]\n",
      "With ReLU: A = [[0.6 ]\n",
      " [0.  ]\n",
      " [1.26]]\n",
      "With softmax: A = \n",
      "[[0.47535001 0.14317267 0.38147732]\n",
      " [0.05272708 0.75380161 0.19347131]\n",
      " [0.68692136 0.05526942 0.25780921]]\n",
      "AL = [[0.56058713]\n",
      " [0.55220559]\n",
      " [0.46331713]]\n",
      "Length of layers list = 2\n",
      "AL = [[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001\n",
      "  0.08448599 0.09067641 0.1294968  0.08303407]\n",
      " [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785\n",
      "  0.08664355 0.09207969 0.12512634 0.08536063]\n",
      " [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299\n",
      "  0.08734059 0.12452515 0.13002144 0.08467232]]\n",
      "Length of layers list = 2\n"
     ]
    }
   ],
   "source": [
    "# Model initialize parameters\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "print(\"W1: \", model.linear[0].parameters[\"W\"], \"\\nb1: \", model.linear[0].parameters[\"b\"])\n",
    "print(\"W2: \", model.linear[1].parameters[\"W\"], \"\\nb2: \", model.linear[1].parameters[\"b\"])\n",
    "\n",
    "# Model forward\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
    "model = Model([3, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "A_prev, W, b = np.array([[4.35, -5.67], [-7.89, 8.12]]), np.array([[-3.54], [-2.34]]), np.array([[0.8]])\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_sigmoid\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
    "model = Model([3, 1], [\"relu\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "A_prev, W, b = np.array([[7.23, -4.56], [5.67, -8.90]]), np.array([[-9.12], [3.45]]), np.array([[0.25]])\n",
    "model = Model([2, 1], [\"relu\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_relu\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1, -0.1, -0.1],[0.2, -0.2, 0.],[0.3, -0.3, 0.1]]), np.array([[-0.5, 0.5, 0.1]])\n",
    "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With softmax: A = \\n\" + str(A))\n",
    "A_prev, W, b = np.array([[-5.12, 4.56, 7.89], [8.34, -6.78, 2.45], [3.21, -4.67, 5.98]]), np.array([[6.23, -7.85, 4.56], [-3.21, 9.87, -2.34], [1.23, -5.67, 8.90]]), np.array([[4.12, -6.54, 7.89]])\n",
    "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_softmax\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "# binary classification\n",
    "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of layers list = \" + str(len(model.linear)))\n",
    "\n",
    "# multi-class classification\n",
    "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
    "model = Model([3, 3, 10], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of layers list = \" + str(len(model.linear)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEmggOxtdMnl"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>W1:</td>\n",
    "    <td>[[ 0.09762701 0.08976637 -0.12482558] [ 0.43037873 -0.1526904 0.783546 ] [ 0.20552675 0.29178823 0.92732552]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1:</td>\n",
    "    <td>[[0. 0. 0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2:</td>\n",
    "    <td>[[-0.20325375] [ 0.53968259] [-1.22446471]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2:</td>\n",
    "    <td>[[0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With Sigmoid:</td>\n",
    "    <td>A = [[0.64565631] [0.20915937] [0.77902611]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With ReLU:</td>\n",
    "    <td>A = [[0.6 ] [0. ] [1.26]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With Softmax:</td>\n",
    "    <td>A = [[0.47535001 0.14317267 0.38147732] [0.05272708 0.75380161 0.19347131] [0.68692136 0.05526942 0.25780921]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AL:</td>\n",
    "    <td>[[0.56058713] [0.55220559] [0.46331713]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Length of layers list:</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AL:</td>\n",
    "    <td>[[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001 0.08448599 0.09067641 0.1294968  0.08303407]\n",
    "         [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785 0.08664355 0.09207969 0.12512634 0.08536063]\n",
    "         [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299 0.08734059 0.12452515 0.13002144 0.08467232]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Length of layers list:</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "HOGsyLXPNGh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.55554938  0.27777469]\n",
      " [ 0.49152369  0.24576184]\n",
      " [-0.41996594 -0.20998297]\n",
      " [-0.55554938 -0.27777469]\n",
      " [-0.39321993 -0.19660997]]\n",
      "dW = [[-0.29446117]\n",
      " [ 0.29446117]]\n",
      "db = [[-0.03216622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[-0.01269296 -0.05595562]\n",
      " [ 0.01470136  0.06480946]\n",
      " [ 0.          0.        ]\n",
      " [-0.07496777 -0.0327431 ]\n",
      " [-0.07151883 -0.03123674]]\n",
      "dW = [[ 0.0178719  -0.17321413]\n",
      " [-0.0178719   0.17321413]]\n",
      "db = [[ 0.00335943 -0.11638953]]\n",
      "\n",
      "Binary classification\n",
      "dW1 = [[-0.06277946  0.26602938 -0.37820327]\n",
      " [ 0.          0.05875647  0.        ]\n",
      " [-0.01569486  0.05181823 -0.09455082]]\n",
      "db1 = [[-0.03138973  0.10363646 -0.18910163]]\n",
      "dA_prev = [[-0.02128713  0.03620889 -0.06919444]\n",
      " [ 0.02675119 -0.04550313  0.08695554]\n",
      " [ 0.08406585 -0.52321654 -0.47247201]]\n",
      "\n",
      "Multi-class classification\n",
      "dW1 = [[ 0.16593371  0.33171007 -0.32297709]\n",
      " [ 0.          0.15006987  0.        ]\n",
      " [ 0.04148343  0.04541005 -0.08074427]]\n",
      "db1 = [[ 0.08296685  0.0908201  -0.16148854]]\n",
      "dA_prev = [[-0.04735391  0.08054785 -0.15392528]\n",
      " [ 0.05429414 -0.09235301  0.1764847 ]\n",
      " [ 0.10229066 -0.30227651 -0.34116033]]\n",
      "\n",
      "Regression\n",
      "dW1 = [[ 0.45352627 -1.49031638  2.73218534]\n",
      " [ 0.          1.09795245  0.        ]\n",
      " [ 0.11338157 -0.64706721  0.68304634]]\n",
      "db1 = [[ 0.22676313 -1.29413441  1.36609267]]\n",
      "dA_prev = [[-0.10931473  0.18594169 -0.35533076]\n",
      " [-0.07704814  0.13105702 -0.25044727]\n",
      " [-0.60730166  3.77977844  3.41319394]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model backward\n",
    "AL, Y, linear_activation_cache = np.array([[0.1], [0.2], [0.5], [0.9], [1.0]]), np.array([[0], [0], [1], [1], [1]]), (((np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[2.0], [1.0]]), np.array([[0.5]])), np.array([[0], [1], [2], [0], [1]])))\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].cache = linear_activation_cache[0]\n",
    "model.activation[0].cache = linear_activation_cache[1]\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(model.linear[0].dW))\n",
    "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
    "AL, Y, linear_activation_cache = np.array([[0.35], [0.93], [0.23], [0.72], [0.90]]), np.array([[1], [0], [1], [0], [1]]), (((np.array([[-1, 2], [1, 3], [2, 0], [1, -4], [3, -2]]), np.array([[1.7], [3.2]]), np.array([[0.25]])), np.array([[2], [1], [2], [0], [0]])))\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].cache = linear_activation_cache[0]\n",
    "model.activation[0].cache = linear_activation_cache[1]\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "outputs[\"model_backward_sigmoid\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
    "\n",
    "X, Y = np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[0], [1], [1], [1], [1]])\n",
    "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(model.linear[0].dW))\n",
    "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
    "X, Y = np.array([[4.56, -3.21], [-7.85, 6.34], [2.45, -8.90], [5.67, 3.12], [-4.78, 7.89]]), np.array([[1], [1], [0], [1], [0]])\n",
    "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "outputs[\"model_backward_relu\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
    "\n",
    "# binary classification\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1], [0], [0]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Binary classification\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
    "\n",
    "# multi-class classification\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "model = Model([3, 3, 3], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Multi-class classification\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
    "\n",
    "# regression - mse\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[2.5], [1.8], [3.2]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"linear\"], \"mse\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Regression\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6xzEk3-NGh6"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Sigmoid</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[ 0.55554938  0.27777469] [ 0.49152369  0.24576184] [-0.41996594 -0.20998297] [-0.55554938 -0.27777469] [-0.39321993 -0.19660997]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW:</td>\n",
    "    <td>[[-0.29446117] [ 0.29446117]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db:</td>\n",
    "    <td>[[-0.03216622]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">ReLU</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.01269296 -0.05595562] [ 0.01470136  0.06480946] [ 0.  0. ] [-0.07496777 -0.0327431 ] [-0.07151883 -0.03123674]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW:</td>\n",
    "    <td>[[ 0.0178719  -0.17321413] [-0.0178719   0.17321413]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db:</td>\n",
    "    <td>[[ 0.00335943 -0.11638953]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Binary Classification</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[-0.06277946  0.26602938 -0.37820327] [ 0.  0.05875647  0. ] [-0.01569486  0.05181823 -0.09455082]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[-0.03138973  0.10363646 -0.18910163]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.02128713  0.03620889 -0.06919444] [ 0.02675119 -0.04550313  0.08695554] [ 0.08406585 -0.52321654 -0.47247201]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Multi-class Classification</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[ 0.16593371  0.33171007 -0.32297709] [ 0.  0.15006987  0. ] [ 0.04148343  0.04541005 -0.08074427]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[ 0.08296685  0.0908201  -0.16148854]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.04735391  0.08054785 -0.15392528] [ 0.05429414 -0.09235301  0.1764847 ] [ 0.10229066 -0.30227651 -0.34116033]]</td>\n",
    "  </tr>\n",
    "  <th colspan=\"2\">Regression</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[ 0.45352627 -1.49031638  2.73218534] [ 0.          1.09795245  0.        ] [ 0.11338157 -0.64706721  0.68304634]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[ 0.22676313 -1.29413441  1.36609267]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.10931473  0.18594169 -0.35533076] [-0.07704814  0.13105702 -0.25044727] [-0.60730166  3.77977844  3.41319394]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "qoGA4O8BUCvq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.39721186  0.07752363  0.392862  ]\n",
      " [ 0.64025004  0.00469968  0.52183369]\n",
      " [-0.09671178  0.09679955  0.33138026]\n",
      " [ 0.27099015  0.33705631  0.67538482]]\n",
      "b1 = [[ 0.16234149  0.78232848 -0.02592894]]\n",
      "W2 = [[0.6012798 ]\n",
      " [0.38575324]\n",
      " [0.49003974]]\n",
      "b2 = [[0.05692437]]\n"
     ]
    }
   ],
   "source": [
    "# Model update\n",
    "np.random.seed(1)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
    "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
    "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "model.update(0.1)\n",
    "print (\"W1 = \"+ str(model.linear[0].parameters[\"W\"]))\n",
    "print (\"b1 = \"+ str(model.linear[0].parameters[\"b\"]))\n",
    "print (\"W2 = \"+ str(model.linear[1].parameters[\"W\"]))\n",
    "print (\"b2 = \"+ str(model.linear[1].parameters[\"b\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
    "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
    "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "model.update(0.075)\n",
    "outputs[\"model_update_parameters\"] = {\"W1\": model.linear[0].parameters[\"W\"], \"b1\": model.linear[0].parameters[\"b\"], \"W2\": model.linear[1].parameters[\"W\"], \"b2\": model.linear[1].parameters[\"b\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t-HfnHZWYIa"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Data Representation</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W1:</td>\n",
    "    <td>[[ 0.39721186  0.07752363  0.392862 ] [ 0.64025004  0.00469968  0.52183369] [-0.09671178  0.09679955  0.33138026] [ 0.27099015  0.33705631  0.67538482]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1:</td>\n",
    "    <td>[[ 0.16234149  0.78232848 -0.02592894]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2:</td>\n",
    "    <td>[[0.6012798 ] [0.38575324] [0.49003974]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2:</td>\n",
    "    <td>[[0.05692437]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSBVaQOSRrk"
   },
   "source": [
    "## **Section 2: Loss function(10%)**\n",
    "In this section, you need to implement the loss function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScdQdj85uC0P"
   },
   "source": [
    "## Part 1: Binary cross-entropy loss (BCE) (5%)\n",
    "Compute the binary cross-entropy loss $L$, using the following formula:  $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}+ϵ\\right)), where\\ ϵ=1e-5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "MjBT0eYQaY81"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_BCE_loss\n",
    "\n",
    "def compute_BCE_loss(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the binary cross-entropy loss function using the above formula.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (n, 1)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (n, 1)\n",
    "\n",
    "    Returns:\n",
    "    loss -- binary cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    eps = 1e-5\n",
    "    loss = -1 / n * np.sum(Y * np.log(AL + eps) + (1 - Y) * np.log(1 - AL + eps))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoV03IzimBEN"
   },
   "source": [
    "### Test your **compute_BCE_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "r07sqnIXaaMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.5783820772863568\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.9], [0.6], [0.4], [0.1], [0.2], [0.8]]), np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "print(\"loss = \" + str(compute_BCE_loss(AL, Y)))\n",
    "outputs[\"compute_BCE_loss\"] = compute_BCE_loss(np.array([[0.12], [0.85], [0.47], [0.33], [0.76], [0.58], [0.09], [0.62]]), np.array([[1], [1], [0], [1], [0], [1], [1], [0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iRtgOx_IGPo"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>loss: </td>\n",
    "    <td>0.5783820772863568</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aealRyKbcQzG"
   },
   "source": [
    "## Part 2: Categorical cross-entropy loss (CCE) (5%)\n",
    "Compute the categorical cross-entropy loss $L$, using the following formula: $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right)),\\ ϵ = 1e-5$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "Owx-kTdcfxV5"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_CCE_loss\n",
    "\n",
    "def compute_CCE_loss(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the categorical cross-entropy loss function using the above formula.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (n, C)\n",
    "    Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "\n",
    "    Returns:\n",
    "    loss -- categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    eps = 1e-5\n",
    "    loss = -1 / n * np.sum(Y * np.log(AL + eps))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSOsacYQmNAb"
   },
   "source": [
    "### Test your **compute_CCE_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "0YbHVAc7hSh3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4722526144672341\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.8, 0.1, 0.1],[0.6, 0.3, 0.1],[0.4, 0.5, 0.1],[0.1, 0.7, 0.2],[0.2, 0.1, 0.7],[0.4, 0.1, 0.5]]), np.array([[1, 0, 0],[1, 0, 0],[0, 1, 0],[0, 1, 0],[0, 0, 1],[0, 0, 1]])\n",
    "print(\"loss = \" + str(compute_CCE_loss(AL, Y)))\n",
    "outputs[\"compute_CCE_loss\"] = compute_CCE_loss(np.array([[0.7, 0.2, 0.1], [0.2, 0.2, 0.6], [0.3, 0.5, 0.2], [0.8, 0.1, 0.1], [0.7, 0.15, 0.15]]), np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9VVIBB5Ic-D"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>loss: </td>\n",
    "    <td>0.4722526144672341</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_XIpJtBpiAX"
   },
   "source": [
    "## Part 3: Mean square error (MSE) (0%)\n",
    "You don't need to write this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "6RHLNGsepygt"
   },
   "outputs": [],
   "source": [
    "# compute_MSE_loss (MSE)\n",
    "def compute_MSE_loss(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    loss = (1/m) * np.sum(np.square(AL - Y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8_AYhzfqlbg"
   },
   "source": [
    "## **Section 3: Training and prediction(35%)**\n",
    "In this section, you will apply your implemented neural network to regression and binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpFQpiK5eF64"
   },
   "source": [
    "## Helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "woCqucFUYXe6"
   },
   "outputs": [],
   "source": [
    "def predict(x, y_true, model):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    x -- data set of examples you would like to label\n",
    "    model -- trained model\n",
    "\n",
    "    Returns:\n",
    "    y_pred -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # Forward propagation\n",
    "    y_pred = model.forward(x)\n",
    "\n",
    "    # this transform the output and label of binary classification when using sigmoid + cross entropy for evaluation\n",
    "    # eg. y_pred: [[0.8], [0.2], [0.1]] -> [[0.2, 0.8], [0.8, 0.2], [0.9, 0.1]]\n",
    "    # eg. y_true: [[1], [0], [0]] -> [[0, 1], [1, 0], [1, 0]]\n",
    "    if y_pred.shape[-1] == 1:\n",
    "        y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
    "        if y_true is not None:\n",
    "            y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
    "\n",
    "    # make y_pred/y_true become one-hot prediction result\n",
    "    # eg. y_true: [[1, 0, 0], [0, 0, 1], [0, 1, 0]] -> [0, 2, 1]\n",
    "    # eg. y_pred: [[0.2, 0.41, 0.39], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]] -> [1, 1, 2]\n",
    "    if y_true is not None:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    if y_true is not None:\n",
    "        # compute accuracy\n",
    "        correct = 0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            if yt == yp:\n",
    "                correct += 1\n",
    "        print(f\"Accuracy: {correct/n * 100:.2f}%\")\n",
    "\n",
    "        f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "        print(f'f1 score for each class: {f1_scores}')\n",
    "        print(f'f1_macro score: {np.mean(np.array(f1_scores)):.2f}')\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def save_prediction_data(predicted_y):\n",
    "    # Create DataFrame with ID, x, and y columns\n",
    "    df = pd.DataFrame({\n",
    "        'ID': range(len(predicted_y)),  # Add ID column starting from 0\n",
    "        'y': predicted_y\n",
    "    })\n",
    "\n",
    "    # Ensure ID is the first column\n",
    "    df = df[['ID', 'y']]\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv('Lab4_basic_regression.csv', index=False)\n",
    "    print(\"Prediction data saved as 'Lab4_basic_regression.csv'\")\n",
    "\n",
    "def animate_training(history, X_train, Y_train):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    line, = ax.plot([], [], 'b-', lw=1, label='Predicted')\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "    ax.plot(ground_truth_x, ground_truth_y, 'r-', lw=1, label='Ground Truth')\n",
    "\n",
    "    # show current epoch on the animation / 100 epoch\n",
    "    epoch_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        epoch_text.set_text('')\n",
    "        return line, epoch_text\n",
    "\n",
    "    def update(frame):\n",
    "        epoch = (frame + 1) * 100\n",
    "        _, predicted_y = history[frame]\n",
    "        predicted_x = X_train.flatten()\n",
    "        line.set_data(predicted_x, predicted_y.flatten())\n",
    "\n",
    "        epoch_text.set_text(f'Epoch: {epoch}')\n",
    "\n",
    "        return line, epoch_text\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(history), init_func=init, blit=True, interval=50)\n",
    "\n",
    "    # save as gif\n",
    "    ani.save('Lab4_basic_regression.gif', writer='pillow')\n",
    "    plt.close(fig)\n",
    "    print(f\"Animation saved as 'Lab4_basic_regression.gif'\")\n",
    "\n",
    "\n",
    "def save_final_result(model, X_train, Y_train):\n",
    "    AL = model.forward(X_train)\n",
    "\n",
    "    predicted_x = X_train.flatten()\n",
    "    predicted_y = AL.flatten()\n",
    "\n",
    "    plt.plot(predicted_x, predicted_y, 'b-', label=\"Predicted\", lw=1)\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "\n",
    "    save_prediction_data(predicted_y)\n",
    "\n",
    "    plt.plot(ground_truth_x, ground_truth_y, 'r-', label='Ground Truth', lw=1)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlim(0, 11)\n",
    "    plt.savefig(\"Lab4_basic_regression.jpg\")\n",
    "    plt.show()\n",
    "    print(\"Prediction saved as 'Lab4_basic_regression.jpg'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgVRVmOYG9FK"
   },
   "source": [
    "## Part1: Training function & batch function (5%)\n",
    "The functions defined in this part will be utilized in the subsequent training parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "fjOBHI0bGVE7"
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n, f^{0})\n",
    "    Y -- true \"label\" vector, of shape (n, C)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size:, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size:, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def train_model(model, X_train, Y_train, learning_rate, num_iterations, batch_size=None, print_loss=True, print_freq=1000, decrease_freq=100, decrease_proportion=0.99):\n",
    "    \"\"\"\n",
    "    Trains the model using mini-batch gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    model -- the model to be trained\n",
    "    X_train -- training set, of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels, of shape (1, m_train)\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    batch_size -- size of a mini batch\n",
    "    print_loss -- if True, print the loss every print_freq iterations\n",
    "    print_freq -- print frequency\n",
    "    decrease_freq -- learning rate decrease frequency\n",
    "    decrease_proportion -- learning rate decrease proportion\n",
    "\n",
    "    Returns:\n",
    "    model -- the trained model\n",
    "    losses -- list of losses computed during the optimization\n",
    "    history -- list of (X_train, Y_pred) tuples for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    history = []\n",
    "    losses = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        ### START CODE HERE ###\n",
    "        # Define mini batches\n",
    "        if batch_size:\n",
    "            mini_batches = random_mini_batches(X_train, Y_train, mini_batch_size=batch_size)\n",
    "        else:\n",
    "            # if batch_size is None, batch is not used, mini_batch = whole dataset\n",
    "            mini_batches = [(X_train, Y_train)]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in mini_batches:\n",
    "            X_batch, Y_batch = batch\n",
    "\n",
    "            # Forward pass\n",
    "            AL = model.forward(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            if model.loss_function == 'cross_entropy':\n",
    "                if model.activation_functions[-1] == \"sigmoid\": # Binary classification\n",
    "                    loss = compute_BCE_loss(AL, Y_batch)\n",
    "                elif model.activation_functions[-1] == \"softmax\": # Multi-class classification\n",
    "                    loss = compute_CCE_loss(AL, Y_batch)\n",
    "            elif model.loss_function == 'mse': # Regression\n",
    "                loss = compute_MSE_loss(AL, Y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            model.backward(AL, Y_batch)\n",
    "\n",
    "            # Update parameters\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        epoch_loss /= len(mini_batches)\n",
    "        losses.append(epoch_loss)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print loss\n",
    "        if print_loss and i % print_freq == 0:\n",
    "            print(f\"Loss after iteration {i}: {epoch_loss}\")\n",
    "\n",
    "        # Store history\n",
    "        if i % 100 == 0:\n",
    "            history.append((X_train, model.forward(X_train)))\n",
    "\n",
    "        # Decrease learning rate\n",
    "        if i % decrease_freq == 0 and i > 0:\n",
    "            learning_rate *= decrease_proportion\n",
    "\n",
    "    return model, losses, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2D8lubAw8pX"
   },
   "source": [
    "## Part 2: Regression (10%)\n",
    "In this part, Your task is to train a neural network model to approximate the following mathematical function:\n",
    "\n",
    "$$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-ksy7v-Hrrt"
   },
   "source": [
    "> ### Step 1: Data generation\n",
    "Generate the mathematical function :  $$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "0yRO6y_FyLPM"
   },
   "outputs": [],
   "source": [
    "def generate_data(num_points=1000):\n",
    "\n",
    "    x = np.linspace(0.01, 11, num_points)\n",
    "    y = np.sin(2 * np.sin(2 * np.sin(2 * np.sin(x))))\n",
    "\n",
    "    return x.reshape(-1, 1), y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC_BxLJJHxHD"
   },
   "source": [
    "> ### Step 2: Train model\n",
    "Implement and train your model using the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "alsJ4F6eHZ2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.8481224082559627\n",
      "Loss after iteration 1000: 0.3876652486623817\n",
      "Loss after iteration 2000: 0.5822268885669453\n",
      "Loss after iteration 3000: 0.40543841821169363\n",
      "Loss after iteration 4000: 0.4239695309384812\n",
      "Loss after iteration 5000: 0.20951020914502427\n",
      "Loss after iteration 6000: 0.16111608733334132\n",
      "Loss after iteration 7000: 0.12657109921071225\n",
      "Loss after iteration 8000: 0.10405473936455868\n",
      "Loss after iteration 9000: 0.07420987279020794\n",
      "Loss after iteration 10000: 0.04863429581999256\n",
      "Loss after iteration 11000: 0.03455373799356246\n",
      "Loss after iteration 12000: 0.010960962639071986\n",
      "Loss after iteration 13000: 0.008629558886831454\n",
      "Loss after iteration 14000: 0.007132911182428226\n",
      "Loss after iteration 15000: 0.006114468813691383\n",
      "Loss after iteration 16000: 0.005388001590256916\n",
      "Loss after iteration 17000: 0.004855571697097299\n",
      "Loss after iteration 18000: 0.0044527423007273996\n",
      "Loss after iteration 19000: 0.004149050214356861\n",
      "Loss after iteration 20000: 0.00390979315727884\n",
      "Loss after iteration 21000: 0.0037200665595985773\n",
      "Loss after iteration 22000: 0.0035702410916601964\n",
      "Loss after iteration 23000: 0.0034495411899000423\n",
      "Loss after iteration 24000: 0.003351538244089542\n",
      "Loss after iteration 25000: 0.0032713268614169167\n",
      "Loss after iteration 26000: 0.003205704136172748\n",
      "Loss after iteration 27000: 0.0031517147062647215\n",
      "Loss after iteration 28000: 0.0031069315554012487\n",
      "Loss after iteration 29000: 0.003069625584795177\n",
      "Loss after iteration 30000: 0.003038259927017925\n",
      "Loss after iteration 31000: 0.003012238778335161\n",
      "Loss after iteration 32000: 0.0029904218475104456\n",
      "Loss after iteration 33000: 0.0029719338693931685\n",
      "Loss after iteration 34000: 0.0029566543123027175\n",
      "Loss after iteration 35000: 0.0029437994472526104\n",
      "Loss after iteration 36000: 0.0029329745578052587\n",
      "Loss after iteration 37000: 0.0029238500482123607\n",
      "Loss after iteration 38000: 0.0029161070303529426\n",
      "Loss after iteration 39000: 0.002909593350571506\n",
      "Loss after iteration 40000: 0.00290409568471324\n",
      "Loss after iteration 41000: 0.0028994683735371683\n",
      "Loss after iteration 42000: 0.0028955796893410557\n",
      "Loss after iteration 43000: 0.002892288778431163\n",
      "Loss after iteration 44000: 0.0028895030244297294\n",
      "Loss after iteration 45000: 0.0028871453589481713\n",
      "Loss after iteration 46000: 0.0028851453318949104\n",
      "Loss after iteration 47000: 0.0028834556235585325\n",
      "Loss after iteration 48000: 0.002882026112462092\n",
      "Loss after iteration 49000: 0.002880818575869167\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "x_train, y_train = generate_data()\n",
    "loss_function = \"mse\"\n",
    "layers_dims = [1, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1]\n",
    "activation_fn = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"linear\"]\n",
    "learning_rate = 0.08\n",
    "num_iterations = 50000\n",
    "print_loss = True\n",
    "print_freq = 1000\n",
    "decrease_freq = 500\n",
    "decrease_proportion = 0.92\n",
    "# You don't necessarily need to use mini_batch in this part\n",
    "batch_size = None\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQQtLyFgL4WD"
   },
   "source": [
    "> ### Step 3: Save prediction\n",
    "Save your model's predictions to:\n",
    "> * *Lab4_basic_regression.csv*\n",
    "> * *Lab4_basic_regression.jpg*\n",
    "> * *Lab4_basic_regression.gif*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "0uwle3uqL9Em"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data saved as 'Lab4_basic_regression.csv'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved as 'Lab4_basic_regression.jpg'\n",
      "Animation saved as 'Lab4_basic_regression.gif'\n"
     ]
    }
   ],
   "source": [
    "save_final_result(model, x_train, y_train)\n",
    "animate_training(history, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX-kDur7xKOW"
   },
   "source": [
    "## Part 3: Binary classification (10%)\n",
    "\n",
    "You will train a model to perform binary classification. Your task is to predict whether an optical coherence tomography (OCT) image shows Choroidal Neovascularization (CNV) or is normal.\n",
    "\n",
    "- Data: OCT scan image of retina\n",
    "- Classes:\n",
    "  - CNV: label = 1\n",
    "  - Normal: label = 0\n",
    "\n",
    "- Data Description:\n",
    "  - Input: Grayscale images (28x28 pixels)\n",
    "  - Training set size: 20000 images\n",
    "  - Testing set size: 5000 images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAvUwG1uSLg_"
   },
   "source": [
    "> ### Step 1: Read data & split data\n",
    "Load *basic_data.npz* and prepare it for training by splitting into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "Hp8M93z7v_lO"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: x=(20000, 784), y=(20000, 1)\n",
      "Test: x=(5000, 784)\n",
      "\n",
      "After splitting:\n",
      "x_train: (16000, 784) | y_train: (16000, 1)\n",
      "x_val: (4000, 784) | y_val: (4000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.load('basic_data.npz')\n",
    "X_train = data[\"x_train\"]\n",
    "Y_train = data[\"y_train\"]\n",
    "X_test = data[\"x_test\"]\n",
    "\n",
    "# Display sample images with labels\n",
    "class_names_binary = {0: 'Normal', 1: 'CNV'}\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "    plt.title(f'Label: {int(Y_train[i])} ({class_names_binary[int(Y_train[i])]})', fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data preprocessing\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Normalize X data to [0,1] range\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Reshape Y_train to 2D array\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot data distribution\n",
    "Y_train_1 = np.sum(Y_train == 1)\n",
    "Y_train_0 = np.sum(Y_train == 0)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.bar([0, 1], [Y_train_0, Y_train_1])\n",
    "plt.title('Data distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print('Train: x=%s, y=%s' % (X_train.shape, Y_train.shape))\n",
    "print('Test: x=%s' % (X_test.shape, ))\n",
    "\n",
    "# Train-validation split\n",
    "### START CODE HERE ###\n",
    "# Choose the ratio for splitting\n",
    "split_ratio = 0.8\n",
    "split_index = int(X_train.shape[0] * split_ratio)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train = X_train[:split_index]\n",
    "y_train = Y_train[:split_index]\n",
    "x_val = X_train[split_index:]\n",
    "y_val = Y_train[split_index:]\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
    "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r01QzzHxeMbR"
   },
   "source": [
    "> ### Step 2: Training and Evaluation\n",
    "Train your model on the prepared OCT image data and evaluate its performance in distinguishing between CNV and normal retinal conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "fI7JY5ESjhZ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.6940152924109638\n",
      "Loss after iteration 200: 0.17839697338977792\n",
      "Loss after iteration 400: 0.08955705123657436\n",
      "Loss after iteration 600: 0.048068068359125374\n",
      "Loss after iteration 800: 0.019635569244592515\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "loss_function = \"cross_entropy\"\n",
    "layers_dims = [28 * 28, 64, 32, 1]\n",
    "activation_fn = [\"relu\", \"relu\", \"sigmoid\"]\n",
    "learning_rate = 0.005\n",
    "num_iterations = 1000\n",
    "print_loss = True\n",
    "print_freq = 200\n",
    "decrease_freq = 200\n",
    "decrease_proportion = 0.9\n",
    "# You might need to use mini_batch to reduce training time in this part\n",
    "batch_size = 64\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "U8q0a20XcPtk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training------\n",
      "Accuracy: 99.98%\n",
      "f1 score for each class: [0.99975134 0.99974865]\n",
      "f1_macro score: 1.00\n",
      "validation------\n",
      "Accuracy: 92.35%\n",
      "f1 score for each class: [0.92193878 0.925     ]\n",
      "f1_macro score: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('training------')\n",
    "pred_train = predict(x_train, y_train, model)\n",
    "print('validation------')\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqtnepD-6I20"
   },
   "source": [
    "> ### Step 3: Save prediction\n",
    "Save your model's predictions to: *Lab4_basic.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "mERo3g41zsyX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data saved as 'Lab4_basic.csv'\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(X_test, None, model)\n",
    "df = pd.DataFrame({\n",
    "    'ID': range(len(pred_test)),\n",
    "    'Label': pred_test.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('Lab4_basic.csv', index=False)\n",
    "print(\"Prediction data saved as 'Lab4_basic.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMCpPFMVdj36"
   },
   "source": [
    "# **Advanced Part (30%)**\n",
    "\n",
    "You will train a model to perform multi-class classification on medical imaging data. Your task is to classify optical coherence tomography (OCT) images of retinal conditions into four different categories.\n",
    "\n",
    "- Data: OCT scan images of retina\n",
    "- Classes:\n",
    "  - CNV (Choroidal Neovascularization): label = 0\n",
    "  - DME (Diabetic Macular Edema): label = 1\n",
    "  - Drusen: label = 2\n",
    "  - Normal: label = 3\n",
    "\n",
    "- Data Description:\n",
    "  - Input: Grayscale images (28x28 pixels)\n",
    "  - Training set size: 37754 images\n",
    "  - Testing set size: 6997 images\n",
    "\n",
    "**Notes:** You can implement other functions to improve your rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_GQ3uO128OC"
   },
   "source": [
    "## Step 1: Read data & split data\n",
    "\n",
    "Load *advanced_data.npz* and prepare it for training by splitting into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "bVSfqnXqXGdC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shapes:\n",
      "Train: X=(37754, 784), Y=(37754,)\n",
      "Test: X=(3000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After preprocessing:\n",
      "shape of X_train: (37754, 784)\n",
      "shape of Y_train: (37754, 4)\n",
      "shape of X_test: (3000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALIAAACqCAYAAAAJDxWeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQB0lEQVR4nO2deZBdxXXGfx8ChDGbhGQhjIXACCpAIgWEWQwusNkTR5AYEFBIcQDhMqTMEgLYlCEsLkxCTJnNEUaWiDGyCAZEWMUOVSxaik1gjMBSGVlIQhIIcABJfPmje0rXw5uZO9t78+6cX9Wrufd03+5zZ77p17dv92nZJgianQ0a7UAQ9AQh5KAShJCDShBCDipBCDmoBCHkoBKEkNtB0sWSftloP4pIuk/SxB4q6wBJrxXOF0o6uCfKzuXNl3RgT5XXHv1eyJJOkDRH0geSlmSh7N8gXyzpw+zLCkkPSzqumMf2EbanlSxrp/by2H7S9i7d9TvXN1XSZa3K3832Yz1Rfkf0ayFLOhu4GvgRMAwYAVwPjGugW6NtbwbsAkwFrpV0UU9XImnDni6zodjulx9gS+AD4Jh28lwM/LJwfhvwNvAe8ASwWyHtSOAV4H1gMfAv2T4E+F/gXWAl8CSwQRv1Gdiple1bwEfA1vn8MeCUfLwT8Hj25x3g19n+RC7rw3yPxwEHAm8B5+V7+O8WW6GuhcAF+T5WAb8ANslp/wg8VctfYBKwBvgk13d3obyD8/FAUqPxx/y5GhiY01p8OwdYBiwBvt2Zv2d/bpH3BTYB7ujENfcBo4AvAPOAWwppNwGn2d4c2B14JNvPIf2RhpJa/e+TBFCWu4ANga/USLsUeBAYBGwHXANg+2s5fbTtzWz/Op9vAwwGtieJrxYnAocBXwZ2Bi7syEHbk0m/iytzfd+ske0HwD7AGGB0vp9i2duQGpcvAicD10ka1FHdLfRnIW8NvGN7bdkLbE+x/b7tj0mt9WhJW+bkNcCukrawvcr2vIJ9OLC97TW5X1payLbXkFrbwTWS15BEua3tj2w/1UFxnwIX2f7Y9v+1keda23+wvRK4HDi+rK8dcCJwie1ltpcD/wacVEhfk9PX2L6X1LKX7r/3ZyGvAIaU7StKGiDpCklvSFpN+tqE1HUA+AdS92KRpMcl7Zvt/w4sAB6U9Kak8zvjpKSNSK35yhrJ/woIeC6PEPxTB8Utt/1RB3n+UDheBGxb2tn22TaX11bZK1o1Kn8CNitbeH8W8tPAx8BRJfOfQHoIPJj0FTgy2wVge7btcaRux53AjGx/3/Y5tncE/g44W9I3OuHnOGAt8FzrBNtv2z7V9rbAacD1HYxUlPkm+FLheASpPwupv71pS4KkbTpZ9h9J3x61yu42/VbItt8Dfkjqix0laVNJG0k6QtKVNS7ZnCT8FaQ/6I9aEiRtLOlESVvmrsBq0tc4kv5W0k6SRHooW9eS1h6SBks6EbgO+LHtFTXyHCNpu3y6iiSmlrKXAjuW+FW05nRJ20kaTOrXtvSvXwB2kzRG0iakrlWRjuq7FbhQ0lBJQ0i/+x4bo++3QgawfRVwNumhYznpa/UMUovamptJX4eLSU/1z7RKPwlYmLsd3yH1CSE9HD5E6vM9DVxv+9F23HpB0gek7sgpwFm2f9hG3r2AZ3P+mcD3bL+Z0y4Gpkl6V9Kx7dTXml+RHiDfBN4ALgOw/TvgknwvrwOt++M3kZ4R3pV0Z41yLwPmAC8CL5Eeli+rka9LqBPPHUHQZ+nXLXJQHULIQSUIIQeVIIQcVIIQclAJqjUDqgRDhgzxyJEjG+1G0AXmzp37ju2hNRN7cXbZFNJMppcLtsHALNI45CxgULYL+Clp7PRFYI/CNRNz/teBiQX7nqTxyAX5WpXxa88993TQnABz3IDZb1OBw1vZzgcetj0KeDifAxxBenEwijQr6wZIb7eAi4C9SbOlLirMiLoBOLVwXeu6gn5ErwnZ9hN8dqLLOKBldcM01s9zGAfcnP/xngG2kjScNJ1wlu2VtleRWvHDc9oWtp/J/6k3U37ORFBB6v2wN8z2knz8Nml+LqQ5qMVZV29lW3v2t2rYg35Kwx72bFtSXd6PS5pEnkg+YsSIz6SPPP+eHq9z4RV/U5d62qqrntTr99ce9W6Rl+ZuAfnnsmxfzJ9PH9wu29qzb1fDXhPbk22PtT126NDaD71Bc1NvIc8kjUKQf95VsE9QYh/gvdwFeQA4VNKg/JB3KPBATlstaZ88PXJCoaygH9JrXQtJt5IWFQ6R9BZp9OEKYIakk0lTIlumF95LWl2xgLQy4NsAtldKuhSYnfNd4rQEB+C7pJGRz5HW0t3XW/cS9H16Tci221rr9ZnVEXnk4fQ2yplCGpNubZ9DWuQZBPGKOqgGIeSgEoSQg0oQQg4qQQg5qAQh5KAShJCDShBCDipBCDmoBCHkoBKEkINKEEIOKkEIOagEIeSgEoSQg0oQQg4qQQg5qAR1F7KkXSQ9X/islnRm3i53ccF+ZOGaCyQtkPSapMMK9sOzbUFnN5kJqkXdwwHYfo201xqSBpBWP99BWqf3E9v/UcwvaVdgPLAbaReghyTtnJOvAw4hxbWYLWmm7VfqcR9B36LRQQy/Abxhe1FaDF2TccB0p73tfi9pAes3T1zgvGeGpOk5bwiZvhFrop40uo88nrTbTwtnSHpR0pRCjLfORiEK+iENE7KkjUn7zt2WTTeQto0dQ9qL+KoerGuSpDmS5ixfvrynig36EI1skY8A5tleCmB7qe11tj8FbmR996GzUYg+Q0Qaqj6NFPLxFLoVLaG0MkcDL+fjmcB4SQMl7UAKIfscKWjLKEk75NZ9fM4b9EMa8rAn6fOk0YbTCuYrJY0h7d65sCXN9nxJM0gPcWuB022vy+WcQQqrNQCYYnt+ve4h6Fs0RMi2PwS2bmU7qY3s2L6ctFN9a/u9pHBbQT+n0aMWQdAjhJCDShBCDipBCDmoBCHkoBKUErKkr5axBUGjKNsiX1PSFgQNod1xZEn7AvsBQyWdXUjagvQSIgj6BB29ENkY2Czn27xgXw18q7ecCoLO0q6QbT8OPC5pqu1FdfIpCDpN2VfUAyVNBkYWr7H99d5wKgg6S1kh3wb8DPg5sK733AmCrlFWyGtt39CrngRBNyg7/Ha3pO9KGi5pcMunVz0Lgk5QtkVu2Xb33ILNwI49604QdI1SQra9Q287EgTdoZSQJU2oZbd9c8+6EwRdo2wfea/C5wDgYtIK6C4haaGkl3JEoTnZNljSLEmv55+Dsl2SfpqjCb0oaY9CORNz/tclTWyrvqD6lO1a/HPxXNJWwPRu1n2Q7XcK5+cDD9u+Ioe/Oh84j7TaelT+7E0KG7B3fti8CBhL6q/PzZGGVnXTr6AJ6eo0zg+Bnu43jwOm5eNpwFEF+81OPANslVdcHwbMsr0yi3cWcHgP+xQ0CWX7yHeTWj1Ik4X+ApjRjXoNPCjJwH/ZngwMs70kp78NDMvHEWko6JCyw2/FwIJrgUW23+pGvfvbXizpC8AsSb8tJtp2FnmPIGkSMAlgxIgRPVVs0Ico1bXIk4d+S5oBNwj4pDuV2l6cfy4jReL8CrC0JUhL/rksZ49IQ0GHlF0hciwpus8xwLHAs5K6NI1T0uclbd5yDBxKiio0k/UvXiYCd+XjmcCEPHqxD/Be7oI8ABwqaVAe4Tg024J+SNmuxQ+AvXILiqShwEPA/3ShzmHAHTmM7IbAr2zfL2k2MEPSycAi0j8MpAAsRwILgD+R4ihje6WkS0mhswAusb2yC/4EFaCskDdoEXFmBV0c8cjxjEfXsK8gxUtubTdwehtlTQGmdMWPoFqUFfL9kh5gfdDB44hQVUEfoqM1ezuRhsXOlfT3wP456Wnglt52LgjK0lGLfDVwAYDt3wC/AZD0lzntm73oWxCUpqN+7jDbL7U2ZtvIXvEoCLpAR0Leqp20z/WgH0HQLToS8hxJp7Y2SjoFmNs7LgVB5+moj3wmacz3RNYLdywp3sXRvehXEHSKjuJaLAX2k3QQsHs232P7kV73LAg6Qdn5yI8Cj/ayL0HQZSKsbFAJQshBJQghB5UghBxUghByUAlCyEElCCEHlSCEHFSCugtZ0pckPSrpFUnzJX0v2y+WtDhHH3pe0pGFay7IkYZek3RYwX54ti3IQV2CfkojNlVfC5xje15ehDpX0qyc9hPbxdADSNoVGA/sBmwLPCRp55x8HXAIKabF7Bxp6JW63EXQp6i7kPMK6CX5+H1Jr9J+YJVxwHTbHwO/l7SAFD4AYEFeA4ik6TlvCLkf0tA+sqSRwF8Dz2bTGTlQ4ZSWIIZEpKGgBA0TsqTNgNuBM22vJgUn/DIwhtRiX9WDdU2SNEfSnOXLl/dUsUEfoiFClrQRScS35LWA2F5qe53tT4EbWd99iEhDQYc0YtRCwE3Aq7b/s2AfXsh2NCn6EKRIQ+MlDZS0Aym87HOkwCyjJO0gaWPSA+HMetxD0PdoxKjFV4GTgJckPZ9t3weOlzSGFKlzIXAagO35kmaQHuLWAqfbXgcg6QxSmKwBwBTb8+t3G0FfohGjFk8BqpHUZsAX25cDl9ew39vedUH/Id7sBZUghBxUghByUAlCyEElCCEHlSCEHFSCEHJQCULIQSUIIQeVIIQcVIIQclAJQshBJQghB5UghBxUghByUAlCyEElCCEHlaDphRzRhgJociFLGkCKNnQEsCtp3d+ujfUqaARNLWRSyIAFtt+0/QnQEm0o6Gc0u5Aj2lAANCYcQN2RNAmYlE8/kPRaF4saArxTqs4fd7GGLtADdZW6rz5wT9u3lb/ZhVwq2pDtycDk7lYmaY7tsd0tp69Rhftq9q5FRBsKgCZvkW2vjWhDATS5kKHu0Ya63T3pozT9fcl2o30Igm7T7H3kIABCyKWp2qvwtjYlalaia1GC/Cr8dxQ23gGOb+aNd3I86uHFTYmAo5r1nqJFLkflXoXbXmJ7Xj5+H+hoU6I+TQi5HJV+FV5jU6KmI4Tcz6mxKVFTEkIuR+mNd5qJWpsSNSsh5HJU7lV4W5sSNSsh5BLYXgu0vAp/FZhRgVfhLZsSfb3W/t/NRgy/BZUgWuSgEoSQg0oQQg4qQQg5qAQh5KAShJD7OJK2kTRd0huS5kq6V9LOkl7u+Or+Q9OvEKky+aXFHcA02+OzbTQwrKGO9UGiRe7bHASssf2zFoPtFyhMYJI0UtKTkublz37ZPlzSE/lFx8uSDpA0QNLUfP6SpLPqf0u9Q7TIfZvdSfOE22MZcIjtjySNAm4FxgInAA/YvjzPp94UGAN80fbuAJK26i3H600IufnZCLhW0hhgHbBzts8GpuSJQXfafl7Sm8COkq4B7gEebITDvUF0Lfo284E9O8hzFrAUGE1qiTcGsP0E8DXSLL2pkibYXpXzPQZ8B/h577hdf0LIfZtHgIE55BcAkv6KP59SuiWwxPanpElAA3K+7YGltm8kCXYPSUOADWzfDlwI7FGf2+h9omvRh7FtSUcDV0s6D/gIWAicWch2PXC7pAnA/cCH2X4gcK6kNcAHwATSqpZfSGppwC7o7XuoFzH7LagE0bUIKkEIOagEIeSgEoSQg0oQQg4qQQg5qAQh5KAShJCDSvD/1LnC3F5zx8sAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After splitting:\n",
      "x_train: (37754, 784) | y_train: (37754, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.load('advanced_data.npz')\n",
    "X_train = data[\"x_train\"]\n",
    "Y_train = data[\"y_train\"]\n",
    "X_test = data[\"x_test\"]\n",
    "\n",
    "print(f'Initial shapes:')\n",
    "print(f'Train: X={X_train.shape}, Y={Y_train.shape}')\n",
    "print(f'Test: X={X_test.shape}')\n",
    "\n",
    "# Display sample images with labels\n",
    "class_names = {0: 'CNV', 1: 'DME', 2: 'Drusen', 3: 'Normal'}\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "    plt.title(f'Label: {int(Y_train[i])} ({class_names[int(Y_train[i])]})', fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data preprocessing\n",
    "### START CODE HERE ###\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 4  # OCT has 4 classes\n",
    "Y_train = np.eye(num_classes)[Y_train.reshape(-1)].reshape(-1, num_classes)\n",
    "\n",
    "# Normalize X data to [0,1] range\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(\"shape of X_train:\", X_train.shape)\n",
    "print(\"shape of Y_train:\", Y_train.shape)\n",
    "print(\"shape of X_test:\", X_test.shape)\n",
    "\n",
    "# Plot class distribution before splitting\n",
    "orig_labels = np.argmax(Y_train, axis=1)\n",
    "unique, counts = np.unique(orig_labels, return_counts=True)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.bar(unique, counts)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Train-validation split\n",
    "### START CODE HERE ###\n",
    "# Choose the ratio for splitting\n",
    "# split_ratio = None\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# training with all dataset\n",
    "x_train = X_train\n",
    "y_train = Y_train\n",
    "# x_val = None\n",
    "# y_val = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
    "# print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngmUDGN13ADi"
   },
   "source": [
    "## Step 2: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "pIi1A-1dFY0u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.3812435236868055\n",
      "Loss after iteration 1000: 0.0030765432288218414\n",
      "Loss after iteration 2000: 0.0020803629973246696\n",
      "Loss after iteration 3000: 0.0020668322724088214\n",
      "Loss after iteration 4000: 0.002060185582186348\n",
      "Loss after iteration 5000: 0.0020600816575993583\n",
      "Loss after iteration 6000: 0.0020553461930845995\n",
      "Loss after iteration 7000: 0.0020545476578944137\n",
      "Loss after iteration 8000: 0.0020541533441429966\n",
      "Loss after iteration 9000: 0.0020558677708498686\n",
      "Loss after iteration 10000: 0.002053879307106245\n",
      "Loss after iteration 11000: 0.0020538367769152824\n",
      "Loss after iteration 12000: 0.0020538179445649908\n",
      "Loss after iteration 13000: 0.0020538076378953138\n",
      "Loss after iteration 14000: 0.002053803180086336\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "loss_function = \"cross_entropy\"\n",
    "layers_dims = [28 * 28, 128, 128, 64, 64, 64, 64, 32, 32, 32, 32, 8, 8, 8, 8, 4]\n",
    "activation_fn = [\"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"relu\", \"softmax\"]\n",
    "learning_rate = 0.05\n",
    "num_iterations = 14400\n",
    "print_loss = True\n",
    "print_freq = 1000\n",
    "decrease_freq = 144\n",
    "decrease_proportion = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehjcfSU2XD3-"
   },
   "outputs": [],
   "source": [
    "print('training------')\n",
    "pred_train = predict(x_train, y_train, model)\n",
    "print('validation------')\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXGnS3HQeNUc"
   },
   "source": [
    "## Step 3: Save prediction\n",
    "Save your model's predictions to: *Lab4_advanced.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "YHFDuq2BQ2qI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data saved as 'Lab4_advanced.csv'\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(X_test, None, model)\n",
    "df = pd.DataFrame({\n",
    "    'ID': range(len(pred_test)),\n",
    "    'Label': pred_test.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('Lab4_advanced.csv', index=False)\n",
    "print(\"Prediction data saved as 'Lab4_advanced.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J91ff4Vk1oB_"
   },
   "source": [
    "# Save outputs\n",
    "Save the outputs of your testing codes to: *Lab4_output.npy*\n",
    "\n",
    "We will test your *Lab4_output.npy* to verify the correctness of your neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "CpxmIFiW1tg9"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert list(outputs.keys()) == [\n",
    "    'dense_forward',\n",
    "    'dense_backward',\n",
    "    'dense_update_parameters',\n",
    "    'sigmoid',\n",
    "    'relu',\n",
    "    'softmax',\n",
    "    'linear',\n",
    "    'sigmoid_backward',\n",
    "    'relu_backward',\n",
    "    'softmax_backward',\n",
    "    'linear_backward',\n",
    "    'model_forward_sigmoid',\n",
    "    'model_forward_relu',\n",
    "    'model_forward_softmax',\n",
    "    'model_backward_sigmoid',\n",
    "    'model_backward_relu',\n",
    "    'model_update_parameters',\n",
    "    'compute_BCE_loss',\n",
    "    'compute_CCE_loss'\n",
    "], \"You're missing something, please restart the kernel and run the code from beginning to the end. If the same error occurs, maybe you deleted some outputs, check the template to find the missing parts!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "uDqCzhsp1yTb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_forward: <class 'tuple'>\n",
      "dense_backward: <class 'tuple'>\n",
      "dense_update_parameters: <class 'dict'>\n",
      "sigmoid: <class 'tuple'>\n",
      "relu: <class 'tuple'>\n",
      "softmax: <class 'tuple'>\n",
      "linear: <class 'tuple'>\n",
      "sigmoid_backward: <class 'numpy.ndarray'>\n",
      "relu_backward: <class 'numpy.ndarray'>\n",
      "softmax_backward: <class 'numpy.ndarray'>\n",
      "linear_backward: <class 'numpy.ndarray'>\n",
      "model_forward_sigmoid: <class 'tuple'>\n",
      "model_forward_relu: <class 'tuple'>\n",
      "model_forward_softmax: <class 'tuple'>\n",
      "model_backward_sigmoid: <class 'tuple'>\n",
      "model_backward_relu: <class 'tuple'>\n",
      "model_update_parameters: <class 'dict'>\n",
      "compute_BCE_loss: <class 'numpy.float64'>\n",
      "compute_CCE_loss: <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "np.save(\"Lab4_output.npy\", outputs)\n",
    "\n",
    "# sanity check for saved outputs\n",
    "submit = np.load(\"Lab4_output.npy\", allow_pickle=True).item()\n",
    "for key, value in submit.items():\n",
    "    print(f\"{key}: {type(value)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
